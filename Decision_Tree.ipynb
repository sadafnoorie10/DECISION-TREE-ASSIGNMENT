{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SwSy287KW8nk",
        "outputId": "935127dc-a737-4516-cb3a-25ee2df401f6"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "hello world!\n"
          ]
        }
      ],
      "source": [
        "print(\"hello world!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Theory"
      ],
      "metadata": {
        "id": "crZPcqP6a0sw"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "1.  What is a Decision Tree, and how does it work.\n",
        "\n",
        "ans. A **Decision Tree** is a supervised machine learning algorithm used for classification and regression tasks. It is a tree-like model of decisions and their possible consequences, where each internal node represents a decision based on an attribute, each branch represents an outcome of that decision, and each leaf node represents a final classification or numerical value.\n",
        "\n",
        "### **How Does a Decision Tree Work?**\n",
        "1. **Splitting:** The dataset is divided into subsets based on an attribute (feature) that provides the best separation according to a criterion like **Gini Index**, **Entropy (Information Gain)**, or **Mean Squared Error (for regression)**.\n",
        "  \n",
        "2. **Decision Nodes:** At each internal node, the algorithm selects the best feature and splits the data accordingly.\n",
        "\n",
        "3. **Leaf Nodes:** When no further splits can be made (based on a stopping condition), the node becomes a leaf, assigning a class label (for classification) or a value (for regression).\n",
        "\n",
        "4. **Pruning (Optional):** To prevent **overfitting**, the tree can be pruned by removing nodes that contribute little to accuracy.\n",
        "\n",
        "### **Example of a Decision Tree**\n",
        "If we want to classify whether a passenger survived the Titanic disaster based on attributes like age, gender, and ticket class:\n",
        "\n",
        "```\n",
        "              [Is Passenger Male?]\n",
        "                 /          \\\n",
        "               Yes          No\n",
        "              /               \\\n",
        "      [Age > 10?]          Survived\n",
        "       /      \\\n",
        "     Yes      No\n",
        "   /     \\\n",
        "  No   Survived\n",
        "Died\n",
        "```\n",
        "Here, the tree makes step-by-step decisions to classify passengers.\n",
        "\n",
        "### **Advantages of Decision Trees**\n",
        "‚úîÔ∏è Easy to interpret and visualize.  \n",
        "‚úîÔ∏è Handles both numerical and categorical data.  \n",
        "‚úîÔ∏è Requires minimal data preprocessing.  \n",
        "\n",
        "### **Disadvantages of Decision Trees**\n",
        "‚ùå Prone to **overfitting** if not pruned properly.  \n",
        "‚ùå Can be **unstable**, meaning small changes in data may lead to a completely different tree.  \n",
        "‚ùå Less effective with complex relationships compared to models like Random Forest or Gradient Boosting.\n"
      ],
      "metadata": {
        "id": "LDRnt5j4XKUo"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "2. What are impurity measures in Decision Trees.\n",
        "\n",
        "ans. ### **Impurity Measures in Decision Trees**\n",
        "Impurity measures help determine how mixed the data is at a given node in a **Decision Tree**. A node is considered **pure** if it contains data points from only one class. The goal of a Decision Tree algorithm is to minimize impurity at each split to create the most effective classification.\n",
        "\n",
        "Here are the most common impurity measures used in Decision Trees:\n",
        "\n",
        "---\n",
        "\n",
        "### **1. Gini Impurity**\n",
        "- Measures the probability of incorrectly classifying a randomly chosen element.\n",
        "- Formula:  \n",
        "  \\[\n",
        "  Gini = 1 - \\sum p_i^2\n",
        "  \\]\n",
        "  Where:\n",
        "  - \\( p_i \\) is the probability of a class in the dataset.\n",
        "\n",
        "#### **Example Calculation**\n",
        "If a node contains:\n",
        "- 60% Class A (\\( p_A = 0.6 \\))\n",
        "- 40% Class B (\\( p_B = 0.4 \\))\n",
        "\n",
        "\\[\n",
        "Gini = 1 - (0.6^2 + 0.4^2) = 1 - (0.36 + 0.16) = 0.48\n",
        "\\]\n",
        "\n",
        "üîπ **Lower Gini = Better split**  \n",
        "üîπ Used in **CART (Classification and Regression Trees)** algorithm.\n",
        "\n",
        "---\n",
        "\n",
        "### **2. Entropy (Information Gain)**\n",
        "- Measures the disorder (randomness) in a dataset.\n",
        "- Formula:  \n",
        "  \\[\n",
        "  Entropy = -\\sum p_i \\log_2(p_i)\n",
        "  \\]\n",
        "\n",
        "#### **Example Calculation**\n",
        "Using the same distribution as before:\n",
        "\\[\n",
        "Entropy = - (0.6 \\log_2 0.6 + 0.4 \\log_2 0.4)\n",
        "\\]\n",
        "\\[\n",
        "= - (0.6 \\times -0.737 + 0.4 \\times -1.322)\n",
        "\\]\n",
        "\\[\n",
        "= 0.971\n",
        "\\]\n",
        "\n",
        "üîπ **Higher Entropy = More impurity**  \n",
        "üîπ **Lower Entropy = Better split**  \n",
        "üîπ Used in **ID3, C4.5, and C5.0** Decision Tree algorithms.\n",
        "\n",
        "---\n",
        "\n",
        "### **3. Mean Squared Error (MSE) ‚Äì For Regression Trees**\n",
        "- Measures variance in numerical outputs.\n",
        "- Formula:\n",
        "  \\[\n",
        "  MSE = \\frac{1}{N} \\sum (y_i - \\bar{y})^2\n",
        "  \\]\n",
        "  Where:\n",
        "  - \\( y_i \\) are actual values.\n",
        "  - \\( \\bar{y} \\) is the mean of values.\n",
        "  - \\( N \\) is the total number of observations.\n",
        "\n",
        "üîπ **Lower MSE = Better split**  \n",
        "üîπ Used in **regression-based Decision Trees**.\n",
        "\n",
        "---\n",
        "\n",
        "### **Choosing the Right Impurity Measure**\n",
        "| Impurity Measure  | Used For | Best For |\n",
        "|------------------|------------|----------------|\n",
        "| **Gini Impurity** | Classification | Faster calculations (CART) |\n",
        "| **Entropy** | Classification | More precise splits (ID3, C4.5) |\n",
        "| **MSE** | Regression | Predicting continuous values |"
      ],
      "metadata": {
        "id": "55ir6zbrXwP1"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "3.  What is the mathematical formula for Gini Impurity.\n",
        "\n",
        "ans. The **mathematical formula** for **Gini Impurity** is:\n",
        "\n",
        "\\[\n",
        "Gini = 1 - \\sum_{i=1}^{C} p_i^2\n",
        "\\]\n",
        "\n",
        "Where:\n",
        "- \\( C \\) is the total number of classes.\n",
        "- \\( p_i \\) is the probability of selecting a data point from class \\( i \\).\n",
        "\n",
        "### **Example Calculation**\n",
        "Suppose a dataset has **two classes**:\n",
        "- Class A: 60% (\\( p_A = 0.6 \\))\n",
        "- Class B: 40% (\\( p_B = 0.4 \\))\n",
        "\n",
        "\\[\n",
        "Gini = 1 - (0.6^2 + 0.4^2)\n",
        "\\]\n",
        "\n",
        "\\[\n",
        "= 1 - (0.36 + 0.16)\n",
        "\\]\n",
        "\n",
        "\\[\n",
        "= 1 - 0.52 = 0.48\n",
        "\\]\n",
        "\n",
        "üîπ **Lower Gini Impurity = Better split**  \n",
        "üîπ **Pure node (only one class) ‚Üí Gini = 0**  \n",
        "üîπ **Maximum impurity (equal distribution of classes) ‚Üí Gini = 0.5 (for two classes)**  "
      ],
      "metadata": {
        "id": "EZBtDBniX2kh"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "4. What is the mathematical formula for Entropy.\n",
        "\n",
        "ans. The **mathematical formula** for **Entropy** in a Decision Tree is:\n",
        "\n",
        "\\[\n",
        "Entropy = -\\sum_{i=1}^{C} p_i \\log_2(p_i)\n",
        "\\]\n",
        "\n",
        "Where:\n",
        "- \\( C \\) is the total number of classes.\n",
        "- \\( p_i \\) is the probability of class \\( i \\).\n",
        "\n",
        "---\n",
        "\n",
        "### **Example Calculation**\n",
        "Suppose a dataset has **two classes**:\n",
        "- Class A: 60% (\\( p_A = 0.6 \\))\n",
        "- Class B: 40% (\\( p_B = 0.4 \\))\n",
        "\n",
        "\\[\n",
        "Entropy = - (0.6 \\log_2 0.6 + 0.4 \\log_2 0.4)\n",
        "\\]\n",
        "\n",
        "Using approximate logarithm values:\n",
        "\\[\n",
        "\\log_2 0.6 \\approx -0.737, \\quad \\log_2 0.4 \\approx -1.322\n",
        "\\]\n",
        "\n",
        "\\[\n",
        "Entropy = - [0.6 \\times (-0.737) + 0.4 \\times (-1.322)]\n",
        "\\]\n",
        "\n",
        "\\[\n",
        "= - [ -0.4422 - 0.5288 ]\n",
        "\\]\n",
        "\n",
        "\\[\n",
        "= 0.971\n",
        "\\]\n",
        "\n",
        "---\n",
        "\n",
        "### **Entropy Values Interpretation**\n",
        "| Class Distribution | Entropy |\n",
        "|--------------------|----------|\n",
        "| (100%, 0%) ‚Üí Pure | **0** |\n",
        "| (50%, 50%) ‚Üí Maximum Impurity | **1** |\n",
        "| (70%, 30%) ‚Üí Some Impurity | **0.88** |\n",
        "| (90%, 10%) ‚Üí Low Impurity | **0.47** |\n",
        "\n",
        "üîπ **Lower Entropy = Better split**  \n",
        "üîπ **Entropy is used in ID3, C4.5, and C5.0 Decision Tree algorithms**  "
      ],
      "metadata": {
        "id": "Z9TKNvGnYAF8"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "5.  What is Information Gain, and how is it used in Decision Trees.\n",
        "\n",
        "ans. ### **What is Information Gain?**\n",
        "**Information Gain (IG)** is a metric used in Decision Trees to measure how much a given feature **reduces uncertainty (entropy)** in the dataset after splitting. It helps the model decide which feature to split on at each step.\n",
        "\n",
        "### **Mathematical Formula for Information Gain**\n",
        "\\[\n",
        "IG = Entropy(Parent) - \\sum_{i=1}^{k} \\frac{|S_i|}{|S|} \\cdot Entropy(S_i)\n",
        "\\]\n",
        "Where:\n",
        "- \\( Entropy(Parent) \\) = Entropy before splitting.\n",
        "- \\( k \\) = Number of subsets after splitting.\n",
        "- \\( S_i \\) = Subset after a split.\n",
        "- \\( |S_i|/|S| \\) = Proportion of data points in subset \\( S_i \\).\n",
        "\n",
        "### **How is Information Gain Used in Decision Trees?**\n",
        "1. **Calculate Entropy of Parent Node**  \n",
        "   - Before splitting, calculate the entropy of the dataset.\n",
        "\n",
        "2. **Split the Data Based on a Feature**  \n",
        "   - Divide the dataset into subsets based on the chosen feature.\n",
        "\n",
        "3. **Calculate Weighted Entropy of Child Nodes**  \n",
        "   - Compute the entropy for each subset and take a weighted sum.\n",
        "\n",
        "4. **Compute Information Gain**  \n",
        "   - Subtract the weighted entropy from the parent entropy.\n",
        "\n",
        "5. **Choose the Best Feature to Split**  \n",
        "   - The feature with the **highest Information Gain** is selected for splitting.\n",
        "\n",
        "---\n",
        "\n",
        "### **Example Calculation**\n",
        "Consider a dataset with **10 samples** labeled as:\n",
        "- **6 positive (‚úîÔ∏è)**\n",
        "- **4 negative (‚ùå)**\n",
        "\n",
        "#### **Step 1: Calculate Entropy of Parent Node**\n",
        "\\[\n",
        "Entropy(Parent) = -\\left(\\frac{6}{10} \\log_2 \\frac{6}{10} + \\frac{4}{10} \\log_2 \\frac{4}{10}\\right)\n",
        "\\]\n",
        "\\[\n",
        "= - (0.6 \\times -0.737 + 0.4 \\times -1.322) = 0.971\n",
        "\\]\n",
        "\n",
        "#### **Step 2: Split the Data**\n",
        "After splitting based on a feature:\n",
        "- **Left Node**: 4 positive, 1 negative ‚Üí \\( Entropy = 0.722 \\)\n",
        "- **Right Node**: 2 positive, 3 negative ‚Üí \\( Entropy = 0.971 \\)\n",
        "\n",
        "#### **Step 3: Calculate Weighted Entropy**\n",
        "\\[\n",
        "Entropy(Children) = \\left(\\frac{5}{10} \\times 0.722\\right) + \\left(\\frac{5}{10} \\times 0.971\\right)\n",
        "\\]\n",
        "\\[\n",
        "= 0.361 + 0.485 = 0.846\n",
        "\\]\n",
        "\n",
        "#### **Step 4: Compute Information Gain**\n",
        "\\[\n",
        "IG = 0.971 - 0.846 = 0.125\n",
        "\\]\n",
        "\n",
        "Since **higher IG is better**, the algorithm selects the feature with the highest IG for the next split.\n",
        "\n",
        "---\n",
        "\n",
        "### **Key Takeaways**\n",
        "‚úÖ **Higher IG = More informative split**  \n",
        "‚úÖ **Used in ID3, C4.5, and C5.0 Decision Trees**  \n",
        "‚úÖ **Entropy-based splits are slower but more precise**  "
      ],
      "metadata": {
        "id": "F42jgpDwYKY2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "6.  What is the difference between Gini Impurity and Entropy.\n",
        "\n",
        "ans. ### **Gini Impurity vs. Entropy: Key Differences**\n",
        "Both **Gini Impurity** and **Entropy** are used to measure the impurity (or disorder) in a dataset when building a **Decision Tree**, but they have some differences in how they calculate impurity and impact the tree-building process.\n",
        "\n",
        "| Feature            | **Gini Impurity** | **Entropy (Information Gain)** |\n",
        "|--------------------|------------------|--------------------------------|\n",
        "| **Formula**       | \\( Gini = 1 - \\sum p_i^2 \\) | \\( Entropy = -\\sum p_i \\log_2 p_i \\) |\n",
        "| **Range**        | **0 to 0.5** (for binary classification) | **0 to 1** (for binary classification) |\n",
        "| **Interpretation** | Measures probability of misclassification | Measures information disorder |\n",
        "| **Computational Cost** | Faster (no logarithms) | Slower (logarithmic calculations) |\n",
        "| **Preference** | Used in **CART (Classification and Regression Trees)** | Used in **ID3, C4.5, and C5.0** |\n",
        "| **When to Use?** | When computational efficiency is important | When precise information gain is needed |\n",
        "\n",
        "---\n",
        "\n",
        "### **Example Comparison**\n",
        "Assume a dataset with **two classes**:  \n",
        "- Class A: 60% (\\( p_A = 0.6 \\))  \n",
        "- Class B: 40% (\\( p_B = 0.4 \\))\n",
        "\n",
        "#### **Gini Impurity Calculation**\n",
        "\\[\n",
        "Gini = 1 - (0.6^2 + 0.4^2)\n",
        "\\]\n",
        "\\[\n",
        "= 1 - (0.36 + 0.16) = 0.48\n",
        "\\]\n",
        "\n",
        "#### **Entropy Calculation**\n",
        "\\[\n",
        "Entropy = - (0.6 \\log_2 0.6 + 0.4 \\log_2 0.4)\n",
        "\\]\n",
        "\\[\n",
        "= - (0.6 \\times -0.737 + 0.4 \\times -1.322) = 0.971\n",
        "\\]\n",
        "\n",
        "---\n",
        "\n",
        "### **Which One Should You Use?**\n",
        "‚úÖ **Use Gini Impurity** if you need faster computations (CART Algorithm).  \n",
        "‚úÖ **Use Entropy** if you want a more information-theoretic approach (ID3, C4.5).  \n",
        "‚úÖ **In practice, both give similar results**, so many implementations default to **Gini** for speed."
      ],
      "metadata": {
        "id": "64adiqTyYXcB"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "7. What is the mathematical explanation behind Decision Trees.\n",
        "\n",
        "ans. ### **Mathematical Explanation Behind Decision Trees**\n",
        "A **Decision Tree** is a supervised learning algorithm that recursively splits data into subsets to minimize impurity and maximize information gain. The mathematics behind Decision Trees revolves around **entropy, Gini impurity, and information gain**.\n",
        "\n",
        "---\n",
        "\n",
        "## **1. Splitting Criteria**\n",
        "At each node, the algorithm selects the best feature to split the data. This decision is based on reducing impurity, which is measured using:\n",
        "\n",
        "### **(A) Entropy (For Information Gain)**\n",
        "\\[\n",
        "Entropy(S) = - \\sum_{i=1}^{C} p_i \\log_2 p_i\n",
        "\\]\n",
        "- \\( S \\) = Dataset at the current node.\n",
        "- \\( C \\) = Number of classes.\n",
        "- \\( p_i \\) = Probability of class \\( i \\).\n",
        "\n",
        "üí° **Lower Entropy = Better Split**\n",
        "\n",
        "### **(B) Gini Impurity**\n",
        "\\[\n",
        "Gini(S) = 1 - \\sum_{i=1}^{C} p_i^2\n",
        "\\]\n",
        "- Measures the probability of misclassification.\n",
        "\n",
        "üí° **Lower Gini = Better Split**\n",
        "\n",
        "---\n",
        "\n",
        "## **2. Information Gain (IG)**\n",
        "To decide which feature to split on, we compute **Information Gain**, which measures the reduction in entropy after a split:\n",
        "\n",
        "\\[\n",
        "IG = Entropy(Parent) - \\sum_{i=1}^{k} \\frac{|S_i|}{|S|} \\cdot Entropy(S_i)\n",
        "\\]\n",
        "Where:\n",
        "- \\( S \\) = Parent dataset.\n",
        "- \\( S_i \\) = Child subset after the split.\n",
        "- \\( |S_i|/|S| \\) = Proportion of data in subset \\( S_i \\).\n",
        "\n",
        "üí° **Higher IG = Better Feature for Splitting**\n",
        "\n",
        "---\n",
        "\n",
        "## **3. Recursive Splitting**\n",
        "The Decision Tree recursively splits the dataset by selecting the feature with the highest Information Gain (or lowest Gini Impurity) at each step until:\n",
        "- A pure node is reached (only one class left).\n",
        "- A stopping criterion is met (e.g., max depth, minimum samples per node).\n",
        "\n",
        "---\n",
        "\n",
        "## **4. Stopping Criteria and Pruning**\n",
        "To prevent **overfitting**, trees are often **pruned**:\n",
        "1. **Pre-Pruning** (Early Stopping)  \n",
        "   - Stop splitting when entropy reduction is too small.\n",
        "2. **Post-Pruning**  \n",
        "   - Build a full tree, then remove branches that add little value.\n",
        "\n",
        "---\n",
        "\n",
        "## **5. Regression Trees (For Continuous Values)**\n",
        "For regression, impurity is measured using **Mean Squared Error (MSE)** instead of entropy:\n",
        "\n",
        "\\[\n",
        "MSE = \\frac{1}{N} \\sum (y_i - \\bar{y})^2\n",
        "\\]\n",
        "- \\( y_i \\) = Actual value.\n",
        "- \\( \\bar{y} \\) = Mean of target values in node.\n",
        "- \\( N \\) = Number of samples.\n",
        "\n",
        "üí° **Lower MSE = Better Split for Regression Trees**\n",
        "\n",
        "---\n",
        "\n",
        "### **Conclusion**\n",
        "1. **Select the best feature** using **Information Gain (Entropy) or Gini Impurity**.\n",
        "2. **Recursively split the data** based on feature values.\n",
        "3. **Stop splitting** when criteria are met (e.g., pure nodes).\n",
        "4. **Prune the tree** to prevent overfitting."
      ],
      "metadata": {
        "id": "IrBSKAdUYjZJ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "8.  What is Pre-Pruning in Decision Trees.\n",
        "\n",
        "ans. ### **Pre-Pruning in Decision Trees (Early Stopping)**\n",
        "Pre-pruning, also known as **early stopping**, is a technique used to **prevent overfitting** in Decision Trees by stopping the tree's growth before it becomes too complex.\n",
        "\n",
        "---\n",
        "\n",
        "### **How Pre-Pruning Works**\n",
        "During tree construction, the algorithm evaluates conditions at each node and **decides whether to stop splitting** based on predefined criteria. If any of these criteria are met, the node is converted into a **leaf node** instead of further splitting.\n",
        "\n",
        "---\n",
        "\n",
        "### **Common Pre-Pruning Techniques**\n",
        "1. **Maximum Depth (Max Depth)**\n",
        "   - Limits how deep the tree can grow.\n",
        "   - Prevents the tree from memorizing training data.\n",
        "   - Example: `max_depth = 5` (Tree stops growing beyond 5 levels).\n",
        "\n",
        "2. **Minimum Samples per Split**\n",
        "   - Ensures each split contains at least a minimum number of samples.\n",
        "   - Example: `min_samples_split = 10` (A node must have at least 10 samples to be split).\n",
        "\n",
        "3. **Minimum Samples per Leaf**\n",
        "   - Each leaf node must have a minimum number of samples.\n",
        "   - Example: `min_samples_leaf = 5` (Each leaf must contain at least 5 samples).\n",
        "\n",
        "4. **Maximum Number of Nodes**\n",
        "   - Limits the total number of nodes in the tree.\n",
        "   - Helps control model complexity.\n",
        "\n",
        "5. **Information Gain Threshold**\n",
        "   - Stops splitting if the **Information Gain** (or Gini decrease) is below a threshold.\n",
        "   - Prevents splitting when the gain is too small.\n",
        "\n",
        "---\n",
        "\n",
        "### **Advantages of Pre-Pruning**\n",
        "‚úÖ Prevents overfitting by limiting tree complexity.  \n",
        "‚úÖ Reduces computation time and improves efficiency.  \n",
        "‚úÖ Helps in better generalization to new data.\n",
        "\n",
        "---\n",
        "\n",
        "### **Disadvantages of Pre-Pruning**\n",
        "‚ùå Risk of **underfitting** if the tree stops too early.  \n",
        "‚ùå Finding the optimal threshold values can be tricky.\n",
        "\n",
        "---\n",
        "\n",
        "### **Comparison with Post-Pruning**\n",
        "| Feature            | **Pre-Pruning** (Early Stopping) | **Post-Pruning** (Pruning After Training) |\n",
        "|--------------------|--------------------------------|----------------------------------|\n",
        "| When?            | Before full tree is built | After full tree is built |\n",
        "| Goal            | Prevents overfitting early | Removes overfitted branches |\n",
        "| Efficiency      | Faster training | Slower, as the full tree is built first |\n",
        "| Risk           | May underfit if stopped too soon | More controlled overfitting reduction |"
      ],
      "metadata": {
        "id": "1v3dGAKmYy2H"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "9.  What is Post-Pruning in Decision Trees.\n",
        "\n",
        "ans. ### **Post-Pruning in Decision Trees (Pruning After Training)**\n",
        "Post-pruning, also called **pruning after training**, is a technique used to **reduce overfitting** by trimming unnecessary branches **after** the full tree has been built.\n",
        "\n",
        "---\n",
        "\n",
        "### **How Post-Pruning Works**\n",
        "1. **Train a full Decision Tree** (allow it to grow completely).\n",
        "2. **Evaluate performance on validation data** to identify overfitted branches.\n",
        "3. **Remove weak or unnecessary nodes** (i.e., nodes that do not improve accuracy significantly).\n",
        "4. **Simplify the tree**, making it more generalized.\n",
        "\n",
        "---\n",
        "\n",
        "### **Post-Pruning Techniques**\n",
        "1. **Cost Complexity Pruning (CCP) ‚Äì CART Algorithm**\n",
        "   - Uses a complexity parameter \\(\\alpha\\) to balance accuracy and tree size.\n",
        "   - Removes branches that increase complexity without much gain in performance.\n",
        "\n",
        "2. **Reduced Error Pruning**\n",
        "   - Removes nodes one by one and checks if accuracy improves or stays the same on validation data.\n",
        "   - If accuracy doesn't decrease, the node is pruned.\n",
        "\n",
        "3. **Minimum Error Pruning**\n",
        "   - Uses statistical tests to prune nodes that contribute minimally to classification accuracy.\n",
        "\n",
        "---\n",
        "\n",
        "### **Mathematical Approach: Cost Complexity Pruning (CCP)**\n",
        "The **Cost Complexity Pruning function** is:\n",
        "\n",
        "\\[\n",
        "R_\\alpha(T) = R(T) + \\alpha \\times |T|\n",
        "\\]\n",
        "\n",
        "Where:\n",
        "- \\( R(T) \\) = Total misclassification error of tree \\( T \\).\n",
        "- \\( |T| \\) = Number of terminal (leaf) nodes.\n",
        "- \\( \\alpha \\) = Complexity parameter (higher \\(\\alpha\\) ‚Üí more pruning).\n",
        "\n",
        "A smaller **\\(\\alpha\\)** keeps more branches, while a larger **\\(\\alpha\\)** removes more branches.\n",
        "\n",
        "---\n",
        "\n",
        "### **Advantages of Post-Pruning**\n",
        "‚úÖ **Better accuracy on unseen data** (reduces overfitting).  \n",
        "‚úÖ **More controlled pruning** (compared to pre-pruning).  \n",
        "‚úÖ **Balances model complexity and performance.**  \n",
        "\n",
        "---\n",
        "\n",
        "### **Disadvantages of Post-Pruning**\n",
        "‚ùå **Computationally expensive** (since a full tree is built first).  \n",
        "‚ùå **Choosing the right pruning level can be tricky.**  \n",
        "\n",
        "---\n",
        "\n",
        "### **Comparison: Pre-Pruning vs. Post-Pruning**\n",
        "| Feature             | **Pre-Pruning** (Early Stopping) | **Post-Pruning** (After Full Tree) |\n",
        "|---------------------|--------------------------------|----------------------------------|\n",
        "| When?              | Stops early during training  | Trims tree after full growth |\n",
        "| Risk               | May **underfit** if stopped too soon | Less risk of underfitting |\n",
        "| Computational Cost  | **Lower** (tree is smaller) | **Higher** (full tree is built first) |\n",
        "| Accuracy           | May not reach best accuracy | Often results in **better generalization** |"
      ],
      "metadata": {
        "id": "Yb9Lk18kZBDP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "10. What is the difference between Pre-Pruning and Post-Pruning.\n",
        "\n",
        "ans. ### **Difference Between Pre-Pruning and Post-Pruning in Decision Trees**  \n",
        "\n",
        "Both **Pre-Pruning** and **Post-Pruning** are techniques used to **prevent overfitting** in Decision Trees, but they differ in **when and how pruning is applied**.\n",
        "\n",
        "---\n",
        "\n",
        "| Feature            | **Pre-Pruning (Early Stopping)** | **Post-Pruning (After Full Tree Growth)** |\n",
        "|--------------------|--------------------------------|----------------------------------|\n",
        "| **When Applied?** | During tree growth (before fully expanding the tree). | After the full tree is built. |\n",
        "| **How It Works?** | Stops tree growth based on conditions (e.g., max depth, min samples per node). | Grows a full tree first, then removes unnecessary branches. |\n",
        "| **Goal** | Prevents overfitting **before** it happens. | Reduces overfitting **after** it happens. |\n",
        "| **Stopping Criteria** | - Maximum tree depth. <br> - Minimum samples per split. <br> - Minimum information gain. | - Prunes branches that do not improve validation accuracy. <br> - Uses **Cost Complexity Pruning (CCP)** or **Reduced Error Pruning**. |\n",
        "| **Computation Cost** | **Lower** (stops tree growth early). | **Higher** (tree is fully built first, then pruned). |\n",
        "| **Risk** | May cause **underfitting** if stopped too soon. | Less risk of underfitting because pruning is based on performance. |\n",
        "| **Example in Scikit-Learn** | `max_depth=5`, `min_samples_split=10`, `min_samples_leaf=5` | `ccp_alpha=0.01` (Cost Complexity Pruning) |\n",
        "\n",
        "---\n",
        "\n",
        "### **Which One to Use?**\n",
        "‚úÖ **Pre-Pruning**: If computation time is a concern (faster but may underfit).  \n",
        "‚úÖ **Post-Pruning**: If accuracy and generalization are more important (slower but better).  \n",
        "\n",
        "In practice, **post-pruning** often results in a **better-performing** tree! üå≥‚ú®  "
      ],
      "metadata": {
        "id": "EGGvQ9YTZQLC"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "11. What is a Decision Tree Regressor.\n",
        "\n",
        "ans. ### **Decision Tree Regressor: An Overview**  \n",
        "A **Decision Tree Regressor** is a machine learning model that predicts **continuous numerical values** instead of categorical labels (used in classification). It works by recursively splitting the dataset based on features, similar to a classification tree, but instead of predicting classes, it predicts **average values** in each region.\n",
        "\n",
        "---\n",
        "\n",
        "### **How Decision Tree Regression Works**\n",
        "1. **Choose the Best Feature to Split**  \n",
        "   - Uses **Mean Squared Error (MSE)**, **Mean Absolute Error (MAE)**, or **Poisson Deviance** to find the best split.\n",
        "2. **Recursive Splitting**  \n",
        "   - The dataset is split into smaller subsets at each node.\n",
        "   - The process continues until a stopping criterion is met.\n",
        "3. **Leaf Nodes Predict the Output**  \n",
        "   - Instead of class labels, leaf nodes store the **mean of target values** in that region.\n",
        "\n",
        "---\n",
        "\n",
        "### **Mathematical Formulation**\n",
        "#### **1. Splitting Criterion: Mean Squared Error (MSE)**\n",
        "At each split, the algorithm minimizes the **MSE**, defined as:\n",
        "\n",
        "\\[\n",
        "MSE = \\frac{1}{n} \\sum_{i=1}^{n} (y_i - \\hat{y})^2\n",
        "\\]\n",
        "\n",
        "where:\n",
        "- \\( y_i \\) = Actual target value.\n",
        "- \\( \\hat{y} \\) = Predicted mean value in the region.\n",
        "- \\( n \\) = Number of samples in that node.\n",
        "\n",
        "The **best split** is the one that results in the **largest reduction in MSE**.\n",
        "\n",
        "#### **2. Prediction at Leaf Nodes**\n",
        "Each leaf node contains the **average** of all target values in that region:\n",
        "\n",
        "\\[\n",
        "\\hat{y} = \\frac{1}{n} \\sum_{i=1}^{n} y_i\n",
        "\\]\n",
        "\n",
        "---\n",
        "\n",
        "### **Example of Decision Tree Regression**\n",
        "Let‚Äôs say we have a dataset where:\n",
        "- **Feature**: Square footage of a house.\n",
        "- **Target**: House price.\n",
        "\n",
        "#### **Splitting Process:**\n",
        "1. The tree first splits based on **square footage** (e.g., \\( \\text{sqft} < 2000 \\)).\n",
        "2. Within each split, it further divides data based on the next best feature (e.g., number of bedrooms).\n",
        "3. Finally, each **leaf node contains the average price** for that region.\n",
        "\n",
        "---\n",
        "\n",
        "### **Hyperparameters in DecisionTreeRegressor**\n",
        "In **Scikit-Learn**, you can control the depth and size of the tree using:\n",
        "- `max_depth`: Limits tree depth to prevent overfitting.\n",
        "- `min_samples_split`: Minimum samples needed to create a new split.\n",
        "- `min_samples_leaf`: Minimum samples per leaf node.\n",
        "- `ccp_alpha`: Controls **post-pruning** (higher values lead to more pruning).\n",
        "\n",
        "---\n",
        "\n",
        "### **Comparison: Decision Tree Regressor vs. Decision Tree Classifier**\n",
        "| Feature                  | **Decision Tree Regressor** | **Decision Tree Classifier** |\n",
        "|--------------------------|----------------------------|------------------------------|\n",
        "| **Output Type**          | Continuous values (e.g., price, temperature). | Discrete classes (e.g., spam/not spam). |\n",
        "| **Splitting Criterion**  | MSE, MAE, Poisson Deviance. | Gini Impurity, Entropy. |\n",
        "| **Leaf Node Prediction** | Mean of target values. | Majority class label. |\n",
        "| **Use Case**            | Predicting prices, stock values, etc. | Classifying emails, images, etc. |"
      ],
      "metadata": {
        "id": "mW1YGLmlZd5L"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "12. What are the advantages and disadvantages of Decision Trees.\n",
        "\n",
        "ans. ### **Advantages and Disadvantages of Decision Trees**  \n",
        "\n",
        "Decision Trees are widely used in machine learning for both **classification** and **regression** tasks. However, they come with their own strengths and weaknesses.  \n",
        "\n",
        "---\n",
        "\n",
        "## ‚úÖ **Advantages of Decision Trees**  \n",
        "\n",
        "### **1. Easy to Understand and Interpret**  \n",
        "- Decision Trees have a clear, **tree-like structure** that is easy to visualize.  \n",
        "- Even non-experts can understand the model‚Äôs decisions.  \n",
        "\n",
        "### **2. Handles Both Numerical and Categorical Data**  \n",
        "- Works well with **continuous variables** (e.g., age, salary) and **categorical variables** (e.g., gender, color).  \n",
        "\n",
        "### **3. No Need for Feature Scaling**  \n",
        "- Unlike models like SVM and Logistic Regression, **Decision Trees don‚Äôt require normalization or standardization**.  \n",
        "\n",
        "### **4. Handles Missing Values Automatically**  \n",
        "- Can split data even when some feature values are missing.  \n",
        "\n",
        "### **5. Works Well with Non-Linear Relationships**  \n",
        "- Unlike linear models, Decision Trees **capture complex patterns and interactions** in data.  \n",
        "\n",
        "### **6. Requires Less Data Preparation**  \n",
        "- No need for **one-hot encoding** for categorical variables.  \n",
        "- No need to remove **outliers** (trees are not sensitive to them).  \n",
        "\n",
        "### **7. Can Be Used for Both Classification and Regression**  \n",
        "- **Decision Tree Classifier** ‚Üí Predicts **discrete labels** (e.g., spam vs. not spam).  \n",
        "- **Decision Tree Regressor** ‚Üí Predicts **continuous values** (e.g., house prices).  \n",
        "\n",
        "---\n",
        "\n",
        "## ‚ùå **Disadvantages of Decision Trees**  \n",
        "\n",
        "### **1. Prone to Overfitting**  \n",
        "- A fully grown tree can become too complex, memorizing training data.  \n",
        "- **Solution**: Use **Pre-Pruning** (e.g., `max_depth`) or **Post-Pruning** (e.g., `ccp_alpha`).  \n",
        "\n",
        "### **2. Unstable (Sensitive to Small Changes in Data)**  \n",
        "- Small changes in data can lead to **completely different** trees.  \n",
        "- **Solution**: Use **Random Forests** (multiple Decision Trees).  \n",
        "\n",
        "### **3. Biased When Data is Imbalanced**  \n",
        "- If one class dominates, the tree may favor it.  \n",
        "- **Solution**: Use **balanced datasets** or apply class weights.  \n",
        "\n",
        "### **4. Not Always the Best for Large Datasets**  \n",
        "- Training time increases with large datasets.  \n",
        "- **Solution**: Use **Random Forests or Gradient Boosting** for better performance.  \n",
        "\n",
        "### **5. Greedy Algorithm (Locally Optimal, Not Globally Optimal)**  \n",
        "- The tree makes **decisions one step at a time**, which may not be the best overall.  \n",
        "- **Solution**: Use **Ensemble Methods (Random Forest, XGBoost)**.  \n",
        "\n",
        "---\n",
        "\n",
        "## **Comparison with Other Models**  \n",
        "\n",
        "| Feature              | **Decision Tree** | **Random Forest** | **Logistic Regression** | **SVM** |\n",
        "|----------------------|------------------|-------------------|-------------------------|---------|\n",
        "| **Overfitting Risk** | High üö® | Low ‚úÖ (Ensemble) | Low ‚úÖ | Low ‚úÖ |\n",
        "| **Computational Cost** | Low ‚úÖ | Medium | Low ‚úÖ | High ‚ùå |\n",
        "| **Handles Non-Linear Data** | Yes ‚úÖ | Yes ‚úÖ | No ‚ùå | Yes ‚úÖ |\n",
        "| **Works on Large Data** | Moderate | Best ‚úÖ | Best ‚úÖ | Slow ‚ùå |\n",
        "| **Interpretability** | Easy ‚úÖ | Hard ‚ùå | Easy ‚úÖ | Hard ‚ùå |\n",
        "\n",
        "---\n",
        "\n",
        "### **Conclusion: When to Use Decision Trees?**  \n",
        "\n",
        "‚úÖ **Best for:**  \n",
        "- Small to medium datasets.  \n",
        "- When interpretability is important.  \n",
        "- When non-linear patterns exist in data.  \n",
        "\n",
        "‚ùå **Avoid when:**  \n",
        "- Overfitting is a concern (use Random Forest instead).  \n",
        "- Dataset is too large (consider Gradient Boosting).  "
      ],
      "metadata": {
        "id": "ORWhut7kZuF-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "13. How does a Decision Tree handle missing values.\n",
        "\n",
        "ans. ### **How Decision Trees Handle Missing Values**  \n",
        "\n",
        "Decision Trees are quite robust when dealing with **missing values**, and they handle them in multiple ways, depending on the implementation.  \n",
        "\n",
        "---\n",
        "\n",
        "### **1Ô∏è‚É£ Ignoring Missing Values in Splitting**  \n",
        "- When deciding on a split, Decision Trees can **ignore missing values** and only use non-missing data.  \n",
        "- This ensures that missing values don‚Äôt affect the split calculation.  \n",
        "\n",
        "üëâ **Example:**  \n",
        "If we are splitting based on \"Age\" and some values are missing, the split will be determined **only using available age values**.  \n",
        "\n",
        "---\n",
        "\n",
        "### **2Ô∏è‚É£ Surrogate Splitting (Used in CART Algorithm)**  \n",
        "- When a feature is missing for a data point, the tree looks for a **backup (surrogate) feature** that gives a similar split.  \n",
        "- This is useful when missing values are **not random** and are related to other features.  \n",
        "\n",
        "üëâ **Example:**  \n",
        "If \"Age\" is missing, but \"Income\" is a good predictor of \"Age,\" the tree may use \"Income\" as a backup split.  \n",
        "\n",
        "---\n",
        "\n",
        "### **3Ô∏è‚É£ Assigning Missing Values to the Most Frequent or Majority Class**  \n",
        "- If a categorical feature is missing, the tree can assign it to the most common category in that node.  \n",
        "- If a numerical feature is missing, it can be replaced with the **mean or median** of that node.  \n",
        "\n",
        "üëâ **Example:**  \n",
        "If \"Gender\" is missing, assign it to the most common gender in the dataset.  \n",
        "\n",
        "---\n",
        "\n",
        "### **4Ô∏è‚É£ Weighted Splitting (Used in Some Implementations)**  \n",
        "- Instead of ignoring missing values, Decision Trees can **distribute** data points with missing values across different branches, based on probability.  \n",
        "- This prevents information loss.  \n",
        "\n",
        "üëâ **Example:**  \n",
        "If 70% of the data goes left and 30% goes right, a missing value might be split **70% left and 30% right**.  \n",
        "\n",
        "---\n",
        "\n",
        "### **5Ô∏è‚É£ Preprocessing: Imputation Before Training**  \n",
        "- If missing values are common, a common approach is to **fill them before training** using:  \n",
        "  - **Mean/Median imputation** (for numerical data).  \n",
        "  - **Mode imputation** (for categorical data).  \n",
        "  - **Using KNN Imputer** (more advanced).  \n",
        "\n",
        "üëâ **Example:**  \n",
        "Using `SimpleImputer` in **Scikit-Learn**:  \n",
        "```python\n",
        "from sklearn.impute import SimpleImputer\n",
        "import numpy as np\n",
        "\n",
        "data = np.array([[25, 50000], [30, np.nan], [35, 60000], [40, 65000]])\n",
        "imputer = SimpleImputer(strategy=\"mean\")  # Replace missing with mean\n",
        "imputed_data = imputer.fit_transform(data)\n",
        "print(imputed_data)\n",
        "```\n",
        "\n",
        "---\n",
        "\n",
        "## **Which Approach is Best?**\n",
        "| Method                  | Pros ‚úÖ | Cons ‚ùå |\n",
        "|-------------------------|--------|---------|\n",
        "| **Ignoring Missing Values** | Simple, fast | Might lose information |\n",
        "| **Surrogate Splitting** | Uses relationships in data | More complex |\n",
        "| **Replacing with Mode/Mean** | Easy, commonly used | May reduce accuracy |\n",
        "| **Weighted Splitting** | Avoids bias | Hard to interpret |\n",
        "\n",
        "---\n",
        "\n",
        "### **Conclusion**  \n",
        "‚úÖ **Decision Trees are flexible with missing values** and can work without explicit imputation.  \n",
        "‚úÖ **For large missing data**, pre-processing (e.g., `SimpleImputer`) can improve performance.  \n",
        "‚úÖ **For complex cases**, Random Forest or boosting methods like XGBoost handle missing values better.  "
      ],
      "metadata": {
        "id": "u7K1qMwiZ8FF"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "14.  How does a Decision Tree handle categorical features.\n",
        "\n",
        "ans. ### **How Decision Trees Handle Categorical Features**  \n",
        "\n",
        "Decision Trees can handle **categorical features** directly, but the approach depends on whether you're using **Classification and Regression Trees (CART)** or **other algorithms like ID3 or C4.5**.  \n",
        "\n",
        "---\n",
        "\n",
        "## **1Ô∏è‚É£ Splitting Categorical Features: Different Methods**  \n",
        "\n",
        "### **(A) One-Hot Encoding (For Most Implementations)**\n",
        "- Converts categorical values into binary columns (`0` or `1`).  \n",
        "- Works well but **increases dimensionality** (not ideal for too many categories).  \n",
        "\n",
        "üëâ **Example:**  \n",
        "Feature: `Color = {Red, Blue, Green}`  \n",
        "One-hot encoding:  \n",
        "```\n",
        "Red    Blue   Green  \n",
        "  1      0      0  \n",
        "  0      1      0  \n",
        "  0      0      1  \n",
        "```\n",
        "\n",
        "‚úÖ **Good for: Algorithms like Scikit-Learn's DecisionTreeClassifier**  \n",
        "‚ùå **Problem:** More categories ‚Üí More columns ‚Üí **Curse of Dimensionality**  \n",
        "\n",
        "**Implementation in Scikit-Learn:**  \n",
        "```python\n",
        "from sklearn.preprocessing import OneHotEncoder\n",
        "import pandas as pd\n",
        "\n",
        "data = pd.DataFrame({'Color': ['Red', 'Blue', 'Green', 'Red']})\n",
        "encoder = OneHotEncoder()\n",
        "encoded_data = encoder.fit_transform(data[['Color']]).toarray()\n",
        "print(encoded_data)\n",
        "```\n",
        "\n",
        "---\n",
        "\n",
        "### **(B) Label Encoding (Simple but Risky)**\n",
        "- Assigns numerical values to categories (e.g., `Red=0`, `Blue=1`, `Green=2`).  \n",
        "- Works for **ordinal** categories (e.g., Small, Medium, Large).  \n",
        "- **Risk:** If used for non-ordinal data, the tree may think `Green (2) > Blue (1)`, which **creates bias**.  \n",
        "\n",
        "üëâ **Example:**  \n",
        "```\n",
        "Color   Encoded\n",
        "Red       0\n",
        "Blue      1\n",
        "Green     2\n",
        "```\n",
        "\n",
        "‚úÖ **Good for:** Ordinal categorical features (like \"Low\", \"Medium\", \"High\").  \n",
        "‚ùå **Bad for:** Non-ordinal categorical features (e.g., color, country names).  \n",
        "\n",
        "**Implementation in Scikit-Learn:**  \n",
        "```python\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "\n",
        "data = ['Red', 'Blue', 'Green', 'Red']\n",
        "encoder = LabelEncoder()\n",
        "encoded_data = encoder.fit_transform(data)\n",
        "print(encoded_data)  # Output: [2 0 1 2]\n",
        "```\n",
        "\n",
        "---\n",
        "\n",
        "### **(C) Direct Categorical Splitting (Used in ID3, C4.5, and Some Decision Trees)**\n",
        "- Decision Trees like **C4.5** can split directly on categorical variables **without encoding**.  \n",
        "- It selects the **best category** for splitting based on **Information Gain** or **Gini Impurity**.  \n",
        "- Each branch represents a category.  \n",
        "\n",
        "üëâ **Example:**  \n",
        "```\n",
        "Feature: Weather = {Sunny, Rainy, Cloudy}\n",
        "If Weather == Sunny ‚Üí Left Branch\n",
        "If Weather == Rainy ‚Üí Right Branch\n",
        "If Weather == Cloudy ‚Üí Another Branch\n",
        "```\n",
        "‚úÖ **Good for:** Algorithms like ID3, C4.5  \n",
        "‚ùå **Not available in** Scikit-Learn's `DecisionTreeClassifier` (requires encoding).  \n",
        "\n",
        "---\n",
        "\n",
        "## **2Ô∏è‚É£ Which Method is Best?**  \n",
        "| **Method**              | **Pros ‚úÖ** | **Cons ‚ùå** |\n",
        "|-------------------------|------------|------------|\n",
        "| **One-Hot Encoding**    | Works well, keeps meaning | High-dimensional for many categories |\n",
        "| **Label Encoding**      | Simple, efficient | Can create false numerical relationships |\n",
        "| **Direct Splitting**    | No need for encoding | Not supported in Scikit-Learn |\n",
        "\n",
        "---\n",
        "\n",
        "## **Conclusion**\n",
        "‚úÖ **For Scikit-Learn Decision Trees:** Use **One-Hot Encoding** (recommended) or **Label Encoding** (only for ordinal data).  \n",
        "‚úÖ **For Algorithms like C4.5 or XGBoost:** Directly split on categorical variables without encoding.  \n",
        "‚úÖ **For Large Categories (e.g., 1000+ unique values):** Consider **Target Encoding** or **Feature Hashing** instead of One-Hot Encoding."
      ],
      "metadata": {
        "id": "Z989E6MLaLkP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "15. What are some real-world applications of Decision Trees?\n",
        "\n",
        "ans. ### **Real-World Applications of Decision Trees** üåçüöÄ  \n",
        "\n",
        "Decision Trees are widely used in various industries because they are **interpretable, efficient, and flexible**. Below are some key applications across different domains:  \n",
        "\n",
        "---\n",
        "\n",
        "## **1Ô∏è‚É£ Business & Marketing**  \n",
        "### **Customer Segmentation**  \n",
        "- Companies use Decision Trees to group customers based on **age, income, purchase history, and behavior**.  \n",
        "- Helps in **personalized marketing** and targeted ads.  \n",
        "\n",
        "üëâ **Example:**  \n",
        "A retail company segments customers into:  \n",
        "- High-value customers üí∞  \n",
        "- Occasional buyers üõí  \n",
        "- Price-sensitive shoppers üíµ  \n",
        "\n",
        "### **Lead Scoring (Sales Prediction)**  \n",
        "- Predicts the likelihood of a customer converting into a paying customer.  \n",
        "- Used in **CRM (Customer Relationship Management) systems**.  \n",
        "\n",
        "üëâ **Example:**  \n",
        "If a customer **visited the website 5+ times, opened an email, and spent 10+ minutes on a product page**, they might be **highly likely to buy**.  \n",
        "\n",
        "---\n",
        "\n",
        "## **2Ô∏è‚É£ Finance & Banking**  \n",
        "### **Credit Scoring & Loan Approval**  \n",
        "- Banks use Decision Trees to assess **loan applications** based on:  \n",
        "  - Credit score üìä  \n",
        "  - Income üí∞  \n",
        "  - Debt-to-income ratio  \n",
        "  - Previous loan repayment history  \n",
        "\n",
        "üëâ **Example:**  \n",
        "A bank may **reject** a loan if:  \n",
        "- Credit score < 600  \n",
        "- Monthly income < ‚Çπ30,000  \n",
        "- History of missed payments  \n",
        "\n",
        "### **Fraud Detection**  \n",
        "- Identifies **unusual transactions** that may indicate fraud.  \n",
        "- Uses customer behavior patterns, transaction amounts, and locations.  \n",
        "\n",
        "üëâ **Example:**  \n",
        "A **‚Çπ2 lakh transaction from a foreign country** when the user has never traveled ‚Üí üö® Potential fraud alert!  \n",
        "\n",
        "---\n",
        "\n",
        "## **3Ô∏è‚É£ Healthcare & Medicine**  \n",
        "### **Disease Diagnosis & Prediction**  \n",
        "- Used in **medical decision-making** to predict diseases based on symptoms.  \n",
        "- Helps doctors diagnose conditions like **diabetes, cancer, and heart disease**.  \n",
        "\n",
        "üëâ **Example:**  \n",
        "A Decision Tree can predict **heart disease** based on:  \n",
        "- High blood pressure  \n",
        "- Cholesterol level  \n",
        "- Family history  \n",
        "- Smoking & alcohol habits üö¨üç∑  \n",
        "\n",
        "### **Medical Treatment Recommendation**  \n",
        "- Suggests the **best treatment** based on patient data.  \n",
        "\n",
        "üëâ **Example:**  \n",
        "If a cancer patient has **Stage 3 lung cancer** and a **history of diabetes**, the Decision Tree recommends **chemotherapy + targeted therapy** instead of surgery.  \n",
        "\n",
        "---\n",
        "\n",
        "## **4Ô∏è‚É£ Aviation & Aerospace** ‚úàÔ∏è  \n",
        "### **Aircraft Maintenance & Fault Detection**  \n",
        "- Used to **predict equipment failure** based on historical sensor data.  \n",
        "- Prevents **costly repairs** and enhances **safety**.  \n",
        "\n",
        "üëâ **Example:**  \n",
        "If engine **vibration levels are abnormal** + **fuel efficiency drops**, the system **flags an issue** for maintenance.  \n",
        "\n",
        "### **Pilot Decision Support Systems**  \n",
        "- Helps pilots make **quick decisions** based on weather, fuel levels, and route conditions.  \n",
        "\n",
        "üëâ **Example:**  \n",
        "If a plane is **low on fuel** + **bad weather is ahead**, the Decision Tree **suggests an emergency landing** at the nearest airport.  \n",
        "\n",
        "---\n",
        "\n",
        "## **5Ô∏è‚É£ Law & Legal Industry ‚öñÔ∏è**  \n",
        "### **Legal Case Prediction**  \n",
        "- Predicts case outcomes based on past **court decisions, evidence, and judge rulings**.  \n",
        "\n",
        "üëâ **Example:**  \n",
        "A Decision Tree may predict **\"Guilty\" or \"Not Guilty\"** based on:  \n",
        "- Strength of evidence  \n",
        "- Eyewitness testimony  \n",
        "- Previous case outcomes  \n",
        "\n",
        "### **Contract Review & Risk Analysis**  \n",
        "- Used by lawyers to **identify risky clauses** in legal contracts.  \n",
        "\n",
        "üëâ **Example:**  \n",
        "If a contract has **hidden penalties + unclear refund policies**, the Decision Tree flags it as **\"High Risk\"**.  \n",
        "\n",
        "---\n",
        "\n",
        "## **6Ô∏è‚É£ Manufacturing & Supply Chain**  \n",
        "### **Quality Control**  \n",
        "- Detects **defective products** on assembly lines.  \n",
        "- Uses Decision Trees to classify **good vs. defective products** based on sensor readings.  \n",
        "\n",
        "üëâ **Example:**  \n",
        "A factory's Decision Tree flags a product **if weight < 50g** and **shape is irregular** ‚Üí üì¢ **\"Defective\"**  \n",
        "\n",
        "### **Inventory Management & Demand Forecasting**  \n",
        "- Helps businesses decide **how much stock to order** based on past sales and seasonal trends.  \n",
        "\n",
        "üëâ **Example:**  \n",
        "If **past sales increased by 30% during Diwali**, the system recommends **higher stock levels in October**.  \n",
        "\n",
        "---\n",
        "\n",
        "## **7Ô∏è‚É£ Energy & Environment**  \n",
        "### **Power Grid Management** ‚ö°  \n",
        "- Predicts **power outages** based on weather, demand, and equipment health.  \n",
        "\n",
        "üëâ **Example:**  \n",
        "If temperature > 40¬∞C + high electricity demand ‚Üí **Increased chance of grid failure**  \n",
        "\n",
        "### **Environmental Monitoring** üåç  \n",
        "- Used to **predict air pollution levels** based on traffic, factories, and weather conditions.  \n",
        "\n",
        "üëâ **Example:**  \n",
        "If **traffic congestion is high + no wind movement**, Decision Tree predicts **\"High Pollution Risk\"** in that area.  \n",
        "\n",
        "---\n",
        "\n",
        "## **8Ô∏è‚É£ E-Commerce & Recommendation Systems**  \n",
        "### **Product Recommendation** üõçÔ∏è  \n",
        "- Predicts what customers might buy based on their past behavior.  \n",
        "\n",
        "üëâ **Example:**  \n",
        "If a user buys a **laptop**, the system suggests:  \n",
        "- **Laptop bag** üéí  \n",
        "- **Wireless mouse** üñ±Ô∏è  \n",
        "- **Cooling pad** ‚ùÑÔ∏è  \n",
        "\n",
        "### **Pricing & Discount Optimization**  \n",
        "- Helps e-commerce websites decide **discount levels** for different customers.  \n",
        "\n",
        "üëâ **Example:**  \n",
        "If a customer **visited a product page 5 times but didn‚Äôt buy**, the system offers a **5% discount** to encourage purchase.  \n",
        "\n",
        "---\n",
        "\n",
        "## **9Ô∏è‚É£ Sports & Gaming üéÆ‚öΩ**  \n",
        "### **Player Performance Analysis**  \n",
        "- Coaches use Decision Trees to analyze **player performance** and suggest improvements.  \n",
        "\n",
        "üëâ **Example:**  \n",
        "If a football player **has low stamina + slow reaction time**, the system recommends **more endurance training**.  \n",
        "\n",
        "### **Game AI & NPC Behavior**  \n",
        "- Used in video games to **control AI characters' decisions** based on player actions.  \n",
        "\n",
        "üëâ **Example:**  \n",
        "If the player **attacks aggressively**, the enemy **adapts and defends** instead of attacking back.  \n",
        "\n",
        "---\n",
        "\n",
        "### **Conclusion** üéØ  \n",
        "‚úÖ Decision Trees are used **everywhere**, from business and healthcare to aviation and gaming.  \n",
        "‚úÖ They **simplify decision-making** and **improve accuracy** in predictions.  \n",
        "‚úÖ When combined with **Random Forests & Gradient Boosting**, they become even more powerful!  "
      ],
      "metadata": {
        "id": "ZOVsfXg1abXg"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Practical"
      ],
      "metadata": {
        "id": "EeJVsekJawZK"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "16. Write a Python program to train a Decision Tree Classifier on the Iris dataset and print the model accuracy.\n",
        "\n",
        "ans. Here‚Äôs a simple **Python program** to train a **Decision Tree Classifier** on the **Iris dataset** and print its accuracy. üöÄ  \n",
        "\n",
        "---\n",
        "\n",
        "### **üìå Steps in the Code:**  \n",
        "1Ô∏è‚É£ Load the **Iris dataset** (preloaded in `sklearn`).  \n",
        "2Ô∏è‚É£ Split the dataset into **training (80%) and testing (20%) sets**.  \n",
        "3Ô∏è‚É£ Train a **Decision Tree Classifier**.  \n",
        "4Ô∏è‚É£ Predict on the **test data** and compute accuracy.  \n",
        "\n",
        "---\n",
        "\n",
        "### **‚úÖ Python Code**\n",
        "```python\n",
        "# Import necessary libraries\n",
        "from sklearn.datasets import load_iris\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Step 1: Load the Iris dataset\n",
        "iris = load_iris()\n",
        "X = iris.data  # Features (sepal length, sepal width, petal length, petal width)\n",
        "y = iris.target  # Target labels (Setosa, Versicolor, Virginica)\n",
        "\n",
        "# Step 2: Split dataset into Training (80%) and Testing (20%) sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Step 3: Train Decision Tree Classifier\n",
        "clf = DecisionTreeClassifier(random_state=42)\n",
        "clf.fit(X_train, y_train)  # Train the model\n",
        "\n",
        "# Step 4: Make Predictions\n",
        "y_pred = clf.predict(X_test)\n",
        "\n",
        "# Step 5: Calculate Accuracy\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "print(f\"Model Accuracy: {accuracy * 100:.2f}%\")\n",
        "```\n",
        "\n",
        "---\n",
        "\n",
        "### **üîπ Output Example:**\n",
        "```\n",
        "Model Accuracy: 100.00%\n",
        "```\n",
        "üí° The **accuracy may vary** based on `random_state` and dataset split.\n",
        "\n",
        "---\n",
        "\n",
        "### **üìå Key Points:**\n",
        "‚úÖ **`DecisionTreeClassifier()`**: Builds a decision tree model.  \n",
        "‚úÖ **`train_test_split()`**: Splits data into training/testing sets.  \n",
        "‚úÖ **`fit()`**: Trains the model.  \n",
        "‚úÖ **`predict()`**: Makes predictions on test data.  \n",
        "‚úÖ **`accuracy_score()`**: Measures how well the model performs.  "
      ],
      "metadata": {
        "id": "VI2mLik1a4yc"
      }
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "h-3XdMGHbMNA"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "17.  Write a Python program to train a Decision Tree Classifier using Gini Impurity as the criterion and print the  feature importances.\n",
        "\n",
        "ans. Here's a **Python program** to train a **Decision Tree Classifier** using **Gini Impurity** as the criterion and print the **feature importances** from the Iris dataset. üöÄ  \n",
        "\n",
        "---\n",
        "\n",
        "### **üìå Steps in the Code:**  \n",
        "1Ô∏è‚É£ Load the **Iris dataset**.  \n",
        "2Ô∏è‚É£ Split into **training (80%) and testing (20%) sets**.  \n",
        "3Ô∏è‚É£ Train a **Decision Tree Classifier** using **Gini Impurity**.  \n",
        "4Ô∏è‚É£ Print the **feature importances**.  \n",
        "\n",
        "---\n",
        "\n",
        "### **‚úÖ Python Code**\n",
        "```python\n",
        "# Import necessary libraries\n",
        "from sklearn.datasets import load_iris\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "import numpy as np\n",
        "\n",
        "# Step 1: Load the Iris dataset\n",
        "iris = load_iris()\n",
        "X = iris.data  # Features (sepal length, sepal width, petal length, petal width)\n",
        "y = iris.target  # Target labels (Setosa, Versicolor, Virginica)\n",
        "\n",
        "# Step 2: Split dataset into Training (80%) and Testing (20%) sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Step 3: Train Decision Tree Classifier using Gini Impurity\n",
        "clf = DecisionTreeClassifier(criterion=\"gini\", random_state=42)\n",
        "clf.fit(X_train, y_train)  # Train the model\n",
        "\n",
        "# Step 4: Print Feature Importances\n",
        "feature_importances = clf.feature_importances_\n",
        "\n",
        "# Display feature importance values\n",
        "print(\"Feature Importances:\")\n",
        "for feature, importance in zip(iris.feature_names, feature_importances):\n",
        "    print(f\"{feature}: {importance:.4f}\")\n",
        "```\n",
        "\n",
        "---\n",
        "\n",
        "### **üîπ Example Output:**\n",
        "```\n",
        "Feature Importances:\n",
        "sepal length (cm): 0.0198\n",
        "sepal width (cm): 0.0000\n",
        "petal length (cm): 0.5651\n",
        "petal width (cm): 0.4151\n",
        "```\n",
        "üí° **Higher importance** means the feature contributes more to classification.\n",
        "\n",
        "---\n",
        "\n",
        "### **üìå Explanation:**\n",
        "‚úÖ **`criterion=\"gini\"`** ‚Üí Uses **Gini Impurity** for splitting.  \n",
        "‚úÖ **`feature_importances_`** ‚Üí Returns importance scores for each feature.  \n",
        "‚úÖ **`zip(iris.feature_names, feature_importances)`** ‚Üí Maps feature names to importance values."
      ],
      "metadata": {
        "id": "HU27C1HCbMrN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "18. Write a Python program to train a Decision Tree Classifier using Entropy as the splitting criterion and print the model accuracy.\n",
        "\n",
        "ans. Here‚Äôs a **Python program** to train a **Decision Tree Classifier** using **Entropy** as the splitting criterion and print the model accuracy. üöÄ  \n",
        "\n",
        "---\n",
        "\n",
        "### **üìå Steps in the Code:**  \n",
        "1Ô∏è‚É£ Load the **Iris dataset**.  \n",
        "2Ô∏è‚É£ Split the dataset into **training (80%) and testing (20%) sets**.  \n",
        "3Ô∏è‚É£ Train a **Decision Tree Classifier** using **Entropy**.  \n",
        "4Ô∏è‚É£ Predict on the test data and compute **accuracy**.  \n",
        "\n",
        "---\n",
        "\n",
        "### **‚úÖ Python Code**\n",
        "```python\n",
        "# Import necessary libraries\n",
        "from sklearn.datasets import load_iris\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Step 1: Load the Iris dataset\n",
        "iris = load_iris()\n",
        "X = iris.data  # Features (sepal length, sepal width, petal length, petal width)\n",
        "y = iris.target  # Target labels (Setosa, Versicolor, Virginica)\n",
        "\n",
        "# Step 2: Split dataset into Training (80%) and Testing (20%) sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Step 3: Train Decision Tree Classifier using Entropy\n",
        "clf = DecisionTreeClassifier(criterion=\"entropy\", random_state=42)\n",
        "clf.fit(X_train, y_train)  # Train the model\n",
        "\n",
        "# Step 4: Make Predictions\n",
        "y_pred = clf.predict(X_test)\n",
        "\n",
        "# Step 5: Calculate Accuracy\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "print(f\"Model Accuracy: {accuracy * 100:.2f}%\")\n",
        "```\n",
        "\n",
        "---\n",
        "\n",
        "### **üîπ Example Output:**\n",
        "```\n",
        "Model Accuracy: 100.00%\n",
        "```\n",
        "üí° The **accuracy may vary** based on `random_state` and dataset split.\n",
        "\n",
        "---\n",
        "\n",
        "### **üìå Explanation:**\n",
        "‚úÖ **`criterion=\"entropy\"`** ‚Üí Uses **Entropy (Information Gain)** for splitting.  \n",
        "‚úÖ **`train_test_split()`** ‚Üí Splits data into **training (80%)** and **testing (20%)**.  \n",
        "‚úÖ **`accuracy_score()`** ‚Üí Measures how well the model performs.  "
      ],
      "metadata": {
        "id": "7U7ZKvLDbffe"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "19. Write a Python program to train a Decision Tree Regressor on a housing dataset and evaluate using Mean squared Error (MSE).\n",
        "\n",
        "ans. Here‚Äôs a **Python program** to train a **Decision Tree Regressor** on a housing dataset and evaluate its performance using **Mean Squared Error (MSE)**. üöÄ  \n",
        "\n",
        "---\n",
        "\n",
        "### **üìå Steps in the Code:**  \n",
        "1Ô∏è‚É£ Load the **California Housing dataset** (from `sklearn`).  \n",
        "2Ô∏è‚É£ Split into **training (80%) and testing (20%) sets**.  \n",
        "3Ô∏è‚É£ Train a **Decision Tree Regressor**.  \n",
        "4Ô∏è‚É£ Predict on the **test data** and compute **MSE**.  \n",
        "\n",
        "---\n",
        "\n",
        "### **‚úÖ Python Code**\n",
        "```python\n",
        "# Import necessary libraries\n",
        "from sklearn.datasets import fetch_california_housing\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.tree import DecisionTreeRegressor\n",
        "from sklearn.metrics import mean_squared_error\n",
        "\n",
        "# Step 1: Load the California Housing dataset\n",
        "housing = fetch_california_housing()\n",
        "X = housing.data  # Features (e.g., median income, house age, population, etc.)\n",
        "y = housing.target  # Target variable (median house value in $100,000s)\n",
        "\n",
        "# Step 2: Split dataset into Training (80%) and Testing (20%) sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Step 3: Train Decision Tree Regressor\n",
        "regressor = DecisionTreeRegressor(random_state=42)\n",
        "regressor.fit(X_train, y_train)  # Train the model\n",
        "\n",
        "# Step 4: Make Predictions\n",
        "y_pred = regressor.predict(X_test)\n",
        "\n",
        "# Step 5: Calculate Mean Squared Error (MSE)\n",
        "mse = mean_squared_error(y_test, y_pred)\n",
        "print(f\"Mean Squared Error (MSE): {mse:.4f}\")\n",
        "```\n",
        "\n",
        "---\n",
        "\n",
        "### **üîπ Example Output:**\n",
        "```\n",
        "Mean Squared Error (MSE): 0.3531\n",
        "```\n",
        "üí° The **MSE may vary** depending on the dataset split and model hyperparameters.\n",
        "\n",
        "---\n",
        "\n",
        "### **üìå Explanation:**\n",
        "‚úÖ **`fetch_california_housing()`** ‚Üí Loads a **real-world housing dataset**.  \n",
        "‚úÖ **`train_test_split()`** ‚Üí Splits data into **training (80%)** and **testing (20%)**.  \n",
        "‚úÖ **`DecisionTreeRegressor()`** ‚Üí Builds a **decision tree-based regression model**.  \n",
        "‚úÖ **`mean_squared_error()`** ‚Üí Evaluates prediction performance.  "
      ],
      "metadata": {
        "id": "zlhq17J_b5Bc"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "20. Write a Python program to train a Decision Tree Classifier and visualize the tree using graphviz.\n",
        "\n",
        "ans. Here‚Äôs a **Python program** to train a **Decision Tree Classifier** and visualize the tree using **Graphviz**. üå≥‚ú®  \n",
        "\n",
        "---\n",
        "\n",
        "### **üìå Steps in the Code:**  \n",
        "1Ô∏è‚É£ Load the **Iris dataset**.  \n",
        "2Ô∏è‚É£ Split into **training (80%) and testing (20%) sets**.  \n",
        "3Ô∏è‚É£ Train a **Decision Tree Classifier**.  \n",
        "4Ô∏è‚É£ **Visualize the Decision Tree** using **Graphviz**.  \n",
        "\n",
        "---\n",
        "\n",
        "### **‚úÖ Python Code**\n",
        "```python\n",
        "# Import necessary libraries\n",
        "from sklearn.datasets import load_iris\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.tree import DecisionTreeClassifier, export_graphviz\n",
        "import graphviz\n",
        "\n",
        "# Step 1: Load the Iris dataset\n",
        "iris = load_iris()\n",
        "X = iris.data  # Features (sepal length, sepal width, petal length, petal width)\n",
        "y = iris.target  # Target labels (Setosa, Versicolor, Virginica)\n",
        "\n",
        "# Step 2: Split dataset into Training (80%) and Testing (20%) sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Step 3: Train Decision Tree Classifier\n",
        "clf = DecisionTreeClassifier(criterion=\"gini\", random_state=42)\n",
        "clf.fit(X_train, y_train)  # Train the model\n",
        "\n",
        "# Step 4: Visualize the Decision Tree using Graphviz\n",
        "dot_data = export_graphviz(\n",
        "    clf,\n",
        "    out_file=None,\n",
        "    feature_names=iris.feature_names,\n",
        "    class_names=iris.target_names,\n",
        "    filled=True,  # Add colors to nodes\n",
        "    rounded=True,  # Rounded boxes for readability\n",
        "    special_characters=True\n",
        ")\n",
        "\n",
        "# Convert to Graphviz format and display\n",
        "graph = graphviz.Source(dot_data)\n",
        "graph.render(\"decision_tree\")  # Saves tree visualization as 'decision_tree.pdf'\n",
        "graph.view()  # Opens the tree visualization\n",
        "\n",
        "# (Optional) Print a success message\n",
        "print(\"Decision Tree has been saved as 'decision_tree.pdf' and displayed.\")\n",
        "```\n",
        "\n",
        "---\n",
        "\n",
        "### **üîπ Example Output:**\n",
        "This code will generate a **Decision Tree visualization** like this:\n",
        "\n",
        "üìÑ The **decision tree** will be saved as `\"decision_tree.pdf\"` and automatically opened for viewing.\n",
        "\n",
        "---\n",
        "\n",
        "### **üìå Explanation:**\n",
        "‚úÖ **`export_graphviz()`** ‚Üí Converts the trained tree into **Graphviz DOT format**.  \n",
        "‚úÖ **`filled=True`** ‚Üí Adds colors to make nodes more readable.  \n",
        "‚úÖ **`graph.render(\"decision_tree\")`** ‚Üí Saves the visualization as a **PDF file**.  \n",
        "‚úÖ **`graph.view()`** ‚Üí Opens the generated PDF file.  \n",
        "\n",
        "---\n",
        "\n",
        "### **üìå Installation Note:**\n",
        "If **Graphviz** is not installed, install it using:\n",
        "```bash\n",
        "pip install graphviz\n",
        "sudo apt-get install graphviz  # For Linux\n",
        "brew install graphviz          # For macOs"
      ],
      "metadata": {
        "id": "Z2gc4S8Fc0eo"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "21.  Write a Python program to train a Decision Tree Classifier with a maximum depth of 3 and compare its accuracy with a fully grown tree.\n",
        "\n",
        "ans. Here‚Äôs a **Python program** to train a **Decision Tree Classifier** with a **maximum depth of 3** and compare its accuracy with a **fully grown tree**. üöÄ  \n",
        "\n",
        "---\n",
        "\n",
        "### **üìå Steps in the Code:**  \n",
        "1Ô∏è‚É£ Load the **Iris dataset**.  \n",
        "2Ô∏è‚É£ Split into **training (80%) and testing (20%) sets**.  \n",
        "3Ô∏è‚É£ Train **two Decision Tree Classifiers**:  \n",
        "   - One with **max depth = 3**  \n",
        "   - One with **no depth restriction (fully grown)**  \n",
        "4Ô∏è‚É£ Predict on the **test data** and compare their **accuracy**.  \n",
        "\n",
        "---\n",
        "\n",
        "### **‚úÖ Python Code**\n",
        "```python\n",
        "# Import necessary libraries\n",
        "from sklearn.datasets import load_iris\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Step 1: Load the Iris dataset\n",
        "iris = load_iris()\n",
        "X = iris.data  # Features (sepal length, sepal width, petal length, petal width)\n",
        "y = iris.target  # Target labels (Setosa, Versicolor, Virginica)\n",
        "\n",
        "# Step 2: Split dataset into Training (80%) and Testing (20%) sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Step 3: Train Decision Tree Classifier with max_depth=3\n",
        "clf_limited = DecisionTreeClassifier(max_depth=3, random_state=42)\n",
        "clf_limited.fit(X_train, y_train)\n",
        "\n",
        "# Step 4: Train Fully Grown Decision Tree Classifier\n",
        "clf_full = DecisionTreeClassifier(random_state=42)  # No max_depth (fully grown)\n",
        "clf_full.fit(X_train, y_train)\n",
        "\n",
        "# Step 5: Make Predictions\n",
        "y_pred_limited = clf_limited.predict(X_test)\n",
        "y_pred_full = clf_full.predict(X_test)\n",
        "\n",
        "# Step 6: Calculate Accuracy\n",
        "accuracy_limited = accuracy_score(y_test, y_pred_limited)\n",
        "accuracy_full = accuracy_score(y_test, y_pred_full)\n",
        "\n",
        "# Step 7: Print Accuracy Comparison\n",
        "print(f\"Accuracy with max_depth=3: {accuracy_limited * 100:.2f}%\")\n",
        "print(f\"Accuracy with fully grown tree: {accuracy_full * 100:.2f}%\")\n",
        "```\n",
        "\n",
        "---\n",
        "\n",
        "### **üîπ Example Output:**\n",
        "```\n",
        "Accuracy with max_depth=3: 96.67%\n",
        "Accuracy with fully grown tree: 100.00%\n",
        "```\n",
        "üí° The **fully grown tree** might overfit, while the **pruned tree** generalizes better.\n",
        "\n",
        "---\n",
        "\n",
        "### **üìå Explanation:**\n",
        "‚úÖ **`max_depth=3`** ‚Üí Limits the tree depth to **avoid overfitting**.  \n",
        "‚úÖ **`DecisionTreeClassifier()`** (no `max_depth`) ‚Üí Creates a **fully grown tree**.  \n",
        "‚úÖ **`accuracy_score()`** ‚Üí Measures and compares model performance.  "
      ],
      "metadata": {
        "id": "jY8fNKSldGuZ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "22. Write a Python program to train a Decision Tree Classifier using min_samples_split=5 and compare its accuracy with a default tree.\n",
        "\n",
        "ans. Here‚Äôs a **Python program** to train a **Decision Tree Classifier** using `min_samples_split=5` and compare its accuracy with a **default decision tree**. üöÄ  \n",
        "\n",
        "---\n",
        "\n",
        "### **üìå Steps in the Code:**  \n",
        "1Ô∏è‚É£ Load the **Iris dataset**.  \n",
        "2Ô∏è‚É£ Split into **training (80%) and testing (20%) sets**.  \n",
        "3Ô∏è‚É£ Train **two Decision Tree Classifiers**:  \n",
        "   - One with **`min_samples_split=5`**  \n",
        "   - One with **default settings**  \n",
        "4Ô∏è‚É£ Predict on the **test data** and compare their **accuracy**.  \n",
        "\n",
        "---\n",
        "\n",
        "### **‚úÖ Python Code**\n",
        "```python\n",
        "# Import necessary libraries\n",
        "from sklearn.datasets import load_iris\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Step 1: Load the Iris dataset\n",
        "iris = load_iris()\n",
        "X = iris.data  # Features (sepal length, sepal width, petal length, petal width)\n",
        "y = iris.target  # Target labels (Setosa, Versicolor, Virginica)\n",
        "\n",
        "# Step 2: Split dataset into Training (80%) and Testing (20%) sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Step 3: Train Decision Tree Classifier with min_samples_split=5\n",
        "clf_limited = DecisionTreeClassifier(min_samples_split=5, random_state=42)\n",
        "clf_limited.fit(X_train, y_train)\n",
        "\n",
        "# Step 4: Train Default Decision Tree Classifier\n",
        "clf_default = DecisionTreeClassifier(random_state=42)  # Default settings\n",
        "clf_default.fit(X_train, y_train)\n",
        "\n",
        "# Step 5: Make Predictions\n",
        "y_pred_limited = clf_limited.predict(X_test)\n",
        "y_pred_default = clf_default.predict(X_test)\n",
        "\n",
        "# Step 6: Calculate Accuracy\n",
        "accuracy_limited = accuracy_score(y_test, y_pred_limited)\n",
        "accuracy_default = accuracy_score(y_test, y_pred_default)\n",
        "\n",
        "# Step 7: Print Accuracy Comparison\n",
        "print(f\"Accuracy with min_samples_split=5: {accuracy_limited * 100:.2f}%\")\n",
        "print(f\"Accuracy with default tree: {accuracy_default * 100:.2f}%\")\n",
        "```\n",
        "\n",
        "---\n",
        "\n",
        "### **üîπ Example Output:**\n",
        "```\n",
        "Accuracy with min_samples_split=5: 96.67%\n",
        "Accuracy with default tree: 100.00%\n",
        "```\n",
        "üí° The **default tree** might overfit, while the **tree with `min_samples_split=5`** may generalize better.\n",
        "\n",
        "---\n",
        "\n",
        "### **üìå Explanation:**\n",
        "‚úÖ **`min_samples_split=5`** ‚Üí Requires **at least 5 samples** to split a node.  \n",
        "‚úÖ **Default tree** ‚Üí May split nodes even if fewer samples exist, leading to **overfitting**.  \n",
        "‚úÖ **`accuracy_score()`** ‚Üí Compares the performance of both models.  "
      ],
      "metadata": {
        "id": "POkxQKNudVHh"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "23.  Write a Python program to apply feature scaling before training a Decision Tree Classifier and compare its  accuracy with unscaled data.\n",
        "\n",
        "ans. Here‚Äôs a **Python program** to apply **feature scaling** before training a **Decision Tree Classifier** and compare its accuracy with an **unscaled dataset**. üöÄ  \n",
        "\n",
        "---\n",
        "\n",
        "### **üìå Steps in the Code:**  \n",
        "1Ô∏è‚É£ Load the **Iris dataset**.  \n",
        "2Ô∏è‚É£ Split into **training (80%) and testing (20%) sets**.  \n",
        "3Ô∏è‚É£ Apply **Standard Scaling** (`StandardScaler` from `sklearn.preprocessing`).  \n",
        "4Ô∏è‚É£ Train **two Decision Tree Classifiers**:  \n",
        "   - One with **scaled features**  \n",
        "   - One with **unscaled features**  \n",
        "5Ô∏è‚É£ Predict on the **test data** and compare their **accuracy**.  \n",
        "\n",
        "---\n",
        "\n",
        "### **‚úÖ Python Code**\n",
        "```python\n",
        "# Import necessary libraries\n",
        "from sklearn.datasets import load_iris\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Step 1: Load the Iris dataset\n",
        "iris = load_iris()\n",
        "X = iris.data  # Features (sepal length, sepal width, petal length, petal width)\n",
        "y = iris.target  # Target labels (Setosa, Versicolor, Virginica)\n",
        "\n",
        "# Step 2: Split dataset into Training (80%) and Testing (20%) sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Step 3: Apply Feature Scaling (Standardization)\n",
        "scaler = StandardScaler()\n",
        "X_train_scaled = scaler.fit_transform(X_train)\n",
        "X_test_scaled = scaler.transform(X_test)\n",
        "\n",
        "# Step 4: Train Decision Tree Classifier on Unscaled Data\n",
        "clf_unscaled = DecisionTreeClassifier(random_state=42)\n",
        "clf_unscaled.fit(X_train, y_train)\n",
        "\n",
        "# Step 5: Train Decision Tree Classifier on Scaled Data\n",
        "clf_scaled = DecisionTreeClassifier(random_state=42)\n",
        "clf_scaled.fit(X_train_scaled, y_train)\n",
        "\n",
        "# Step 6: Make Predictions\n",
        "y_pred_unscaled = clf_unscaled.predict(X_test)\n",
        "y_pred_scaled = clf_scaled.predict(X_test_scaled)\n",
        "\n",
        "# Step 7: Calculate Accuracy\n",
        "accuracy_unscaled = accuracy_score(y_test, y_pred_unscaled)\n",
        "accuracy_scaled = accuracy_score(y_test, y_pred_scaled)\n",
        "\n",
        "# Step 8: Print Accuracy Comparison\n",
        "print(f\"Accuracy without Feature Scaling: {accuracy_unscaled * 100:.2f}%\")\n",
        "print(f\"Accuracy with Feature Scaling: {accuracy_scaled * 100:.2f}%\")\n",
        "```\n",
        "\n",
        "---\n",
        "\n",
        "### **üîπ Example Output:**\n",
        "```\n",
        "Accuracy without Feature Scaling: 100.00%\n",
        "Accuracy with Feature Scaling: 100.00%\n",
        "```\n",
        "üí° The accuracy may **not change** significantly because **Decision Trees are not sensitive to feature scaling**.\n",
        "\n",
        "---\n",
        "\n",
        "### **üìå Explanation:**\n",
        "‚úÖ **`StandardScaler()`** ‚Üí Standardizes features to **zero mean** and **unit variance**.  \n",
        "‚úÖ **Decision Trees** do **not require scaling**, unlike models like **Logistic Regression** or **SVM**.  \n",
        "‚úÖ This comparison **demonstrates that feature scaling has no impact on Decision Trees**.  "
      ],
      "metadata": {
        "id": "ZlmlPt80djS6"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "24. Write a Python program to train a Decision Tree Classifier using One-vs-Rest (OvR) strategy for multiclass  classification.\n",
        "\n",
        "ans. Here‚Äôs a **Python program** to train a **Decision Tree Classifier** using the **One-vs-Rest (OvR) strategy** for multiclass classification. üöÄ  \n",
        "\n",
        "---\n",
        "\n",
        "### **üìå Steps in the Code:**  \n",
        "1Ô∏è‚É£ Load the **Iris dataset**.  \n",
        "2Ô∏è‚É£ Split into **training (80%) and testing (20%) sets**.  \n",
        "3Ô∏è‚É£ Use **One-vs-Rest (OvR)** with `OneVsRestClassifier` from `sklearn.multiclass`.  \n",
        "4Ô∏è‚É£ Train a **Decision Tree Classifier** using the OvR strategy.  \n",
        "5Ô∏è‚É£ Predict on the **test data** and calculate **accuracy**.  \n",
        "\n",
        "---\n",
        "\n",
        "### **‚úÖ Python Code**\n",
        "```python\n",
        "# Import necessary libraries\n",
        "from sklearn.datasets import load_iris\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.multiclass import OneVsRestClassifier\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Step 1: Load the Iris dataset\n",
        "iris = load_iris()\n",
        "X = iris.data  # Features (sepal length, sepal width, petal length, petal width)\n",
        "y = iris.target  # Target labels (Setosa, Versicolor, Virginica)\n",
        "\n",
        "# Step 2: Split dataset into Training (80%) and Testing (20%) sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Step 3: Apply One-vs-Rest (OvR) Strategy with Decision Tree Classifier\n",
        "ovr_clf = OneVsRestClassifier(DecisionTreeClassifier(random_state=42))\n",
        "ovr_clf.fit(X_train, y_train)\n",
        "\n",
        "# Step 4: Make Predictions\n",
        "y_pred = ovr_clf.predict(X_test)\n",
        "\n",
        "# Step 5: Calculate Accuracy\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "\n",
        "# Step 6: Print the Results\n",
        "print(f\"Accuracy using One-vs-Rest (OvR) with Decision Tree: {accuracy * 100:.2f}%\")\n",
        "```\n",
        "\n",
        "---\n",
        "\n",
        "### **üîπ Example Output:**\n",
        "```\n",
        "Accuracy using One-vs-Rest (OvR) with Decision Tree: 100.00%\n",
        "```\n",
        "üí° The **Iris dataset** is simple, so **Decision Trees can achieve 100% accuracy** in many cases.\n",
        "\n",
        "---\n",
        "\n",
        "### **üìå Explanation:**\n",
        "‚úÖ **One-vs-Rest (OvR) Strategy** ‚Üí Splits the **multiclass problem into multiple binary classification problems**.  \n",
        "‚úÖ **Each class is treated as \"One\" (positive class) vs. \"Rest\" (all other classes combined as negative class)**.  \n",
        "‚úÖ **`OneVsRestClassifier()`** ‚Üí Wraps the **DecisionTreeClassifier** for OvR classification."
      ],
      "metadata": {
        "id": "wbs8g7imdy4F"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "25. Write a Python program to train a Decision Tree Classifier and display the feature importance scores.\n",
        "\n",
        "ans. Here‚Äôs a **Python program** to train a **Decision Tree Classifier** and display the **feature importance scores**. üöÄ  \n",
        "\n",
        "---\n",
        "\n",
        "### **üìå Steps in the Code:**  \n",
        "1Ô∏è‚É£ Load the **Iris dataset**.  \n",
        "2Ô∏è‚É£ Split into **training (80%) and testing (20%) sets**.  \n",
        "3Ô∏è‚É£ Train a **Decision Tree Classifier**.  \n",
        "4Ô∏è‚É£ Extract and display **feature importance scores**.  \n",
        "\n",
        "---\n",
        "\n",
        "### **‚úÖ Python Code**\n",
        "```python\n",
        "# Import necessary libraries\n",
        "from sklearn.datasets import load_iris\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "import pandas as pd\n",
        "\n",
        "# Step 1: Load the Iris dataset\n",
        "iris = load_iris()\n",
        "X = iris.data  # Features (sepal length, sepal width, petal length, petal width)\n",
        "y = iris.target  # Target labels (Setosa, Versicolor, Virginica)\n",
        "\n",
        "# Step 2: Split dataset into Training (80%) and Testing (20%) sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Step 3: Train Decision Tree Classifier\n",
        "clf = DecisionTreeClassifier(random_state=42)\n",
        "clf.fit(X_train, y_train)\n",
        "\n",
        "# Step 4: Get Feature Importance Scores\n",
        "feature_importances = clf.feature_importances_\n",
        "\n",
        "# Step 5: Display Feature Importance in a Table\n",
        "feature_names = iris.feature_names\n",
        "importance_df = pd.DataFrame({'Feature': feature_names, 'Importance': feature_importances})\n",
        "importance_df = importance_df.sort_values(by='Importance', ascending=False)\n",
        "\n",
        "# Print Feature Importance Scores\n",
        "print(\"Feature Importance Scores:\")\n",
        "print(importance_df)\n",
        "```\n",
        "\n",
        "---\n",
        "\n",
        "### **üîπ Example Output:**\n",
        "```\n",
        "Feature Importance Scores:\n",
        "         Feature  Importance\n",
        "2  petal length      0.67\n",
        "3  petal width       0.30\n",
        "0  sepal length      0.03\n",
        "1  sepal width       0.00\n",
        "```\n",
        "üí° The **most important features** have higher values, indicating they contribute more to decision-making.\n",
        "\n",
        "---\n",
        "\n",
        "### **üìå Explanation:**\n",
        "‚úÖ **`feature_importances_`** ‚Üí Extracts the importance of each feature in the decision tree.  \n",
        "‚úÖ **Higher values** indicate a **greater impact** on classification decisions.  \n",
        "‚úÖ **Sorting the values** helps identify the **most influential features**.  "
      ],
      "metadata": {
        "id": "0SdAHc1YeC1A"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "26. Write a Python program to train a Decision Tree Regressor with max_depth=5 and compare its performance  with an unrestricted tree.\n",
        "\n",
        "ans. Here‚Äôs a **Python program** to train a **Decision Tree Regressor** with `max_depth=5` and compare its performance with an **unrestricted tree** using **Mean Squared Error (MSE)**. üöÄ  \n",
        "\n",
        "---\n",
        "\n",
        "### **üìå Steps in the Code:**  \n",
        "1Ô∏è‚É£ Load the **California Housing dataset**.  \n",
        "2Ô∏è‚É£ Split into **training (80%) and testing (20%) sets**.  \n",
        "3Ô∏è‚É£ Train **two Decision Tree Regressors**:  \n",
        "   - One with **`max_depth=5`**  \n",
        "   - One with **default settings (unrestricted depth)**  \n",
        "4Ô∏è‚É£ Predict on the **test data** and compare their **MSE (Mean Squared Error)**.  \n",
        "\n",
        "---\n",
        "\n",
        "### **‚úÖ Python Code**\n",
        "```python\n",
        "# Import necessary libraries\n",
        "from sklearn.datasets import fetch_california_housing\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.tree import DecisionTreeRegressor\n",
        "from sklearn.metrics import mean_squared_error\n",
        "\n",
        "# Step 1: Load the California Housing dataset\n",
        "housing = fetch_california_housing()\n",
        "X = housing.data  # Features\n",
        "y = housing.target  # Target: Median house value\n",
        "\n",
        "# Step 2: Split dataset into Training (80%) and Testing (20%) sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Step 3: Train Decision Tree Regressor with max_depth=5\n",
        "regressor_limited = DecisionTreeRegressor(max_depth=5, random_state=42)\n",
        "regressor_limited.fit(X_train, y_train)\n",
        "\n",
        "# Step 4: Train Unrestricted Decision Tree Regressor\n",
        "regressor_unrestricted = DecisionTreeRegressor(random_state=42)  # No depth restriction\n",
        "regressor_unrestricted.fit(X_train, y_train)\n",
        "\n",
        "# Step 5: Make Predictions\n",
        "y_pred_limited = regressor_limited.predict(X_test)\n",
        "y_pred_unrestricted = regressor_unrestricted.predict(X_test)\n",
        "\n",
        "# Step 6: Calculate Mean Squared Error (MSE)\n",
        "mse_limited = mean_squared_error(y_test, y_pred_limited)\n",
        "mse_unrestricted = mean_squared_error(y_test, y_pred_unrestricted)\n",
        "\n",
        "# Step 7: Print MSE Comparison\n",
        "print(f\"MSE with max_depth=5: {mse_limited:.4f}\")\n",
        "print(f\"MSE with unrestricted tree: {mse_unrestricted:.4f}\")\n",
        "```\n",
        "\n",
        "---\n",
        "\n",
        "### **üîπ Example Output:**\n",
        "```\n",
        "MSE with max_depth=5: 0.4992\n",
        "MSE with unrestricted tree: 0.2927\n",
        "```\n",
        "üí° The **unrestricted tree** usually has a **lower MSE** but may be **overfitting**, while the **max_depth=5 tree** generalizes better.\n",
        "\n",
        "---\n",
        "\n",
        "### **üìå Explanation:**\n",
        "‚úÖ **`max_depth=5`** ‚Üí Limits the depth to prevent overfitting and improve generalization.  \n",
        "‚úÖ **Unrestricted tree** ‚Üí Can grow **too deep**, leading to **overfitting**.  \n",
        "‚úÖ **`mean_squared_error()`** ‚Üí Measures how far the predictions are from actual values.  "
      ],
      "metadata": {
        "id": "Np6KHiMDeSqX"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "27. write a Python program to train a Decision Tree Classifier, apply Cost Complexity Pruning (CCP), and visualize its effect on accuracy.\n",
        "\n",
        "ans. Here‚Äôs a **Python program** to train a **Decision Tree Classifier**, apply **Cost Complexity Pruning (CCP)**, and visualize its effect on accuracy. üöÄ  \n",
        "\n",
        "---\n",
        "\n",
        "### **üìå Steps in the Code:**  \n",
        "1Ô∏è‚É£ Load the **Iris dataset**.  \n",
        "2Ô∏è‚É£ Split into **training (80%) and testing (20%) sets**.  \n",
        "3Ô∏è‚É£ Train a **Decision Tree Classifier** without pruning.  \n",
        "4Ô∏è‚É£ Extract **effective Œ± (CCP alpha) values** using `cost_complexity_pruning_path()`.  \n",
        "5Ô∏è‚É£ Train multiple **pruned Decision Trees** with different **CCP alphas**.  \n",
        "6Ô∏è‚É£ Plot **alpha vs. accuracy** to see how pruning affects performance.  \n",
        "\n",
        "---\n",
        "\n",
        "### **‚úÖ Python Code**\n",
        "```python\n",
        "# Import necessary libraries\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.datasets import load_iris\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Step 1: Load the Iris dataset\n",
        "iris = load_iris()\n",
        "X = iris.data  # Features\n",
        "y = iris.target  # Target labels\n",
        "\n",
        "# Step 2: Split dataset into Training (80%) and Testing (20%) sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Step 3: Train a Decision Tree Classifier without pruning\n",
        "clf = DecisionTreeClassifier(random_state=42)\n",
        "clf.fit(X_train, y_train)\n",
        "\n",
        "# Step 4: Get Cost Complexity Pruning Path\n",
        "path = clf.cost_complexity_pruning_path(X_train, y_train)\n",
        "ccp_alphas = path.ccp_alphas  # Different pruning strengths\n",
        "\n",
        "# Step 5: Train Decision Trees for each CCP alpha and store accuracy\n",
        "train_accuracies = []\n",
        "test_accuracies = []\n",
        "\n",
        "for alpha in ccp_alphas:\n",
        "    pruned_clf = DecisionTreeClassifier(random_state=42, ccp_alpha=alpha)\n",
        "    pruned_clf.fit(X_train, y_train)\n",
        "    \n",
        "    y_train_pred = pruned_clf.predict(X_train)\n",
        "    y_test_pred = pruned_clf.predict(X_test)\n",
        "    \n",
        "    train_accuracies.append(accuracy_score(y_train, y_train_pred))\n",
        "    test_accuracies.append(accuracy_score(y_test, y_test_pred))\n",
        "\n",
        "# Step 6: Plot CCP Alpha vs. Accuracy\n",
        "plt.figure(figsize=(8, 5))\n",
        "plt.plot(ccp_alphas, train_accuracies, marker=\"o\", label=\"Training Accuracy\", linestyle=\"dashed\")\n",
        "plt.plot(ccp_alphas, test_accuracies, marker=\"s\", label=\"Testing Accuracy\", linestyle=\"solid\")\n",
        "plt.xlabel(\"CCP Alpha\")\n",
        "plt.ylabel(\"Accuracy\")\n",
        "plt.title(\"Effect of Cost Complexity Pruning on Decision Tree Accuracy\")\n",
        "plt.legend()\n",
        "plt.grid()\n",
        "plt.show()\n",
        "```\n",
        "\n",
        "---\n",
        "\n",
        "### **üîπ Expected Output:**\n",
        "- A **plot** showing how accuracy changes with different **CCP alpha values**.\n",
        "- **Low Œ± (minimal pruning)** ‚Üí High training accuracy but potential **overfitting**.\n",
        "- **High Œ± (strong pruning)** ‚Üí Simpler tree but potential **underfitting**.\n",
        "\n",
        "---\n",
        "\n",
        "### **üìå Explanation:**\n",
        "‚úÖ **`cost_complexity_pruning_path()`** ‚Üí Extracts **Œ± values** for pruning.  \n",
        "‚úÖ **`ccp_alpha`** ‚Üí Controls pruning strength (higher Œ± means more pruning).  \n",
        "‚úÖ **Plotting accuracy vs. Œ±** ‚Üí Helps find the **optimal balance between overfitting and underfitting**.\n",
        "\n"
      ],
      "metadata": {
        "id": "p2ItuT47ehwA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Import necessary libraries\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.tree import DecisionTreeClassifier, export_text, export_graphviz\n",
        "import graphviz\n",
        "from sklearn.datasets import load_iris\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# Step 1: Load the Iris dataset\n",
        "iris = load_iris()\n",
        "X = iris.data\n",
        "y = iris.target\n",
        "\n",
        "# Step 2: Split dataset into Training (80%) and Testing (20%) sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Step 3: Train Decision Trees with different ccp_alpha values\n",
        "alphas = [0.0, 0.01, 0.05]  # Different pruning levels\n",
        "trees = {}\n",
        "\n",
        "for alpha in alphas:\n",
        "    clf = DecisionTreeClassifier(random_state=42, ccp_alpha=alpha)\n",
        "    clf.fit(X_train, y_train)\n",
        "    trees[alpha] = clf\n",
        "\n",
        "# Step 4: Visualize Trees with Graphviz\n",
        "for alpha, clf in trees.items():\n",
        "    dot_data = export_graphviz(\n",
        "        clf, out_file=None, feature_names=iris.feature_names,\n",
        "        class_names=iris.target_names, filled=True, rounded=True, special_characters=True\n",
        "    )\n",
        "\n",
        "    print(f\"Visualization of Decision Tree with ccp_alpha = {alpha}\")\n",
        "    display(graphviz.Source(dot_data))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "3RUD6mVcfCpZ",
        "outputId": "bba7d8c3-b659-4ba8-c65c-e6a05d083ce8"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Visualization of Decision Tree with ccp_alpha = 0.0\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "image/svg+xml": "<?xml version=\"1.0\" encoding=\"UTF-8\" standalone=\"no\"?>\n<!DOCTYPE svg PUBLIC \"-//W3C//DTD SVG 1.1//EN\"\n \"http://www.w3.org/Graphics/SVG/1.1/DTD/svg11.dtd\">\n<!-- Generated by graphviz version 2.43.0 (0)\n -->\n<!-- Title: Tree Pages: 1 -->\n<svg width=\"751pt\" height=\"790pt\"\n viewBox=\"0.00 0.00 751.00 790.00\" xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">\n<g id=\"graph0\" class=\"graph\" transform=\"scale(1 1) rotate(0) translate(4 786)\">\n<title>Tree</title>\n<polygon fill=\"white\" stroke=\"transparent\" points=\"-4,4 -4,-786 747,-786 747,4 -4,4\"/>\n<!-- 0 -->\n<g id=\"node1\" class=\"node\">\n<title>0</title>\n<path fill=\"#fdfffd\" stroke=\"black\" d=\"M266,-782C266,-782 131,-782 131,-782 125,-782 119,-776 119,-770 119,-770 119,-711 119,-711 119,-705 125,-699 131,-699 131,-699 266,-699 266,-699 272,-699 278,-705 278,-711 278,-711 278,-770 278,-770 278,-776 272,-782 266,-782\"/>\n<text text-anchor=\"start\" x=\"127\" y=\"-766.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">petal length (cm) ‚â§ 2.45</text>\n<text text-anchor=\"start\" x=\"163\" y=\"-751.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">gini = 0.667</text>\n<text text-anchor=\"start\" x=\"153.5\" y=\"-736.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">samples = 120</text>\n<text text-anchor=\"start\" x=\"140.5\" y=\"-721.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">value = [40, 41, 39]</text>\n<text text-anchor=\"start\" x=\"146\" y=\"-706.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">class = versicolor</text>\n</g>\n<!-- 1 -->\n<g id=\"node2\" class=\"node\">\n<title>1</title>\n<path fill=\"#e58139\" stroke=\"black\" d=\"M167,-655.5C167,-655.5 74,-655.5 74,-655.5 68,-655.5 62,-649.5 62,-643.5 62,-643.5 62,-599.5 62,-599.5 62,-593.5 68,-587.5 74,-587.5 74,-587.5 167,-587.5 167,-587.5 173,-587.5 179,-593.5 179,-599.5 179,-599.5 179,-643.5 179,-643.5 179,-649.5 173,-655.5 167,-655.5\"/>\n<text text-anchor=\"start\" x=\"92.5\" y=\"-640.3\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">gini = 0.0</text>\n<text text-anchor=\"start\" x=\"79.5\" y=\"-625.3\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">samples = 40</text>\n<text text-anchor=\"start\" x=\"70\" y=\"-610.3\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">value = [40, 0, 0]</text>\n<text text-anchor=\"start\" x=\"77\" y=\"-595.3\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">class = setosa</text>\n</g>\n<!-- 0&#45;&gt;1 -->\n<g id=\"edge1\" class=\"edge\">\n<title>0&#45;&gt;1</title>\n<path fill=\"none\" stroke=\"black\" d=\"M171.44,-698.91C163.93,-687.65 155.78,-675.42 148.24,-664.11\"/>\n<polygon fill=\"black\" stroke=\"black\" points=\"151.07,-662.05 142.61,-655.67 145.25,-665.93 151.07,-662.05\"/>\n<text text-anchor=\"middle\" x=\"137.71\" y=\"-676.48\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">True</text>\n</g>\n<!-- 2 -->\n<g id=\"node3\" class=\"node\">\n<title>2</title>\n<path fill=\"#f5fef9\" stroke=\"black\" d=\"M344,-663C344,-663 209,-663 209,-663 203,-663 197,-657 197,-651 197,-651 197,-592 197,-592 197,-586 203,-580 209,-580 209,-580 344,-580 344,-580 350,-580 356,-586 356,-592 356,-592 356,-651 356,-651 356,-657 350,-663 344,-663\"/>\n<text text-anchor=\"start\" x=\"205\" y=\"-647.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">petal length (cm) ‚â§ 4.75</text>\n<text text-anchor=\"start\" x=\"248.5\" y=\"-632.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">gini = 0.5</text>\n<text text-anchor=\"start\" x=\"235.5\" y=\"-617.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">samples = 80</text>\n<text text-anchor=\"start\" x=\"222\" y=\"-602.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">value = [0, 41, 39]</text>\n<text text-anchor=\"start\" x=\"224\" y=\"-587.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">class = versicolor</text>\n</g>\n<!-- 0&#45;&gt;2 -->\n<g id=\"edge2\" class=\"edge\">\n<title>0&#45;&gt;2</title>\n<path fill=\"none\" stroke=\"black\" d=\"M225.56,-698.91C231.43,-690.1 237.7,-680.7 243.76,-671.61\"/>\n<polygon fill=\"black\" stroke=\"black\" points=\"246.85,-673.28 249.49,-663.02 241.03,-669.4 246.85,-673.28\"/>\n<text text-anchor=\"middle\" x=\"254.39\" y=\"-683.84\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">False</text>\n</g>\n<!-- 3 -->\n<g id=\"node4\" class=\"node\">\n<title>3</title>\n<path fill=\"#3ee684\" stroke=\"black\" d=\"M255.5,-544C255.5,-544 125.5,-544 125.5,-544 119.5,-544 113.5,-538 113.5,-532 113.5,-532 113.5,-473 113.5,-473 113.5,-467 119.5,-461 125.5,-461 125.5,-461 255.5,-461 255.5,-461 261.5,-461 267.5,-467 267.5,-473 267.5,-473 267.5,-532 267.5,-532 267.5,-538 261.5,-544 255.5,-544\"/>\n<text text-anchor=\"start\" x=\"121.5\" y=\"-528.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">petal width (cm) ‚â§ 1.65</text>\n<text text-anchor=\"start\" x=\"155\" y=\"-513.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">gini = 0.053</text>\n<text text-anchor=\"start\" x=\"149.5\" y=\"-498.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">samples = 37</text>\n<text text-anchor=\"start\" x=\"140\" y=\"-483.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">value = [0, 36, 1]</text>\n<text text-anchor=\"start\" x=\"138\" y=\"-468.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">class = versicolor</text>\n</g>\n<!-- 2&#45;&gt;3 -->\n<g id=\"edge3\" class=\"edge\">\n<title>2&#45;&gt;3</title>\n<path fill=\"none\" stroke=\"black\" d=\"M246.66,-579.91C240.13,-571.01 233.14,-561.51 226.39,-552.33\"/>\n<polygon fill=\"black\" stroke=\"black\" points=\"229.03,-550.01 220.28,-544.02 223.39,-554.15 229.03,-550.01\"/>\n</g>\n<!-- 6 -->\n<g id=\"node7\" class=\"node\">\n<title>6</title>\n<path fill=\"#9253e8\" stroke=\"black\" d=\"M427.5,-544C427.5,-544 297.5,-544 297.5,-544 291.5,-544 285.5,-538 285.5,-532 285.5,-532 285.5,-473 285.5,-473 285.5,-467 291.5,-461 297.5,-461 297.5,-461 427.5,-461 427.5,-461 433.5,-461 439.5,-467 439.5,-473 439.5,-473 439.5,-532 439.5,-532 439.5,-538 433.5,-544 427.5,-544\"/>\n<text text-anchor=\"start\" x=\"293.5\" y=\"-528.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">petal width (cm) ‚â§ 1.75</text>\n<text text-anchor=\"start\" x=\"327\" y=\"-513.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">gini = 0.206</text>\n<text text-anchor=\"start\" x=\"321.5\" y=\"-498.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">samples = 43</text>\n<text text-anchor=\"start\" x=\"312\" y=\"-483.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">value = [0, 5, 38]</text>\n<text text-anchor=\"start\" x=\"314\" y=\"-468.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">class = virginica</text>\n</g>\n<!-- 2&#45;&gt;6 -->\n<g id=\"edge6\" class=\"edge\">\n<title>2&#45;&gt;6</title>\n<path fill=\"none\" stroke=\"black\" d=\"M306.34,-579.91C312.87,-571.01 319.86,-561.51 326.61,-552.33\"/>\n<polygon fill=\"black\" stroke=\"black\" points=\"329.61,-554.15 332.72,-544.02 323.97,-550.01 329.61,-554.15\"/>\n</g>\n<!-- 4 -->\n<g id=\"node5\" class=\"node\">\n<title>4</title>\n<path fill=\"#39e581\" stroke=\"black\" d=\"M109,-417.5C109,-417.5 12,-417.5 12,-417.5 6,-417.5 0,-411.5 0,-405.5 0,-405.5 0,-361.5 0,-361.5 0,-355.5 6,-349.5 12,-349.5 12,-349.5 109,-349.5 109,-349.5 115,-349.5 121,-355.5 121,-361.5 121,-361.5 121,-405.5 121,-405.5 121,-411.5 115,-417.5 109,-417.5\"/>\n<text text-anchor=\"start\" x=\"32.5\" y=\"-402.3\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">gini = 0.0</text>\n<text text-anchor=\"start\" x=\"19.5\" y=\"-387.3\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">samples = 36</text>\n<text text-anchor=\"start\" x=\"10\" y=\"-372.3\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">value = [0, 36, 0]</text>\n<text text-anchor=\"start\" x=\"8\" y=\"-357.3\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">class = versicolor</text>\n</g>\n<!-- 3&#45;&gt;4 -->\n<g id=\"edge4\" class=\"edge\">\n<title>3&#45;&gt;4</title>\n<path fill=\"none\" stroke=\"black\" d=\"M145.4,-460.91C132.28,-449.1 117.96,-436.22 104.89,-424.45\"/>\n<polygon fill=\"black\" stroke=\"black\" points=\"107.13,-421.76 97.35,-417.67 102.44,-426.96 107.13,-421.76\"/>\n</g>\n<!-- 5 -->\n<g id=\"node6\" class=\"node\">\n<title>5</title>\n<path fill=\"#8139e5\" stroke=\"black\" d=\"M240,-417.5C240,-417.5 151,-417.5 151,-417.5 145,-417.5 139,-411.5 139,-405.5 139,-405.5 139,-361.5 139,-361.5 139,-355.5 145,-349.5 151,-349.5 151,-349.5 240,-349.5 240,-349.5 246,-349.5 252,-355.5 252,-361.5 252,-361.5 252,-405.5 252,-405.5 252,-411.5 246,-417.5 240,-417.5\"/>\n<text text-anchor=\"start\" x=\"167.5\" y=\"-402.3\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">gini = 0.0</text>\n<text text-anchor=\"start\" x=\"158\" y=\"-387.3\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">samples = 1</text>\n<text text-anchor=\"start\" x=\"148.5\" y=\"-372.3\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">value = [0, 0, 1]</text>\n<text text-anchor=\"start\" x=\"147\" y=\"-357.3\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">class = virginica</text>\n</g>\n<!-- 3&#45;&gt;5 -->\n<g id=\"edge5\" class=\"edge\">\n<title>3&#45;&gt;5</title>\n<path fill=\"none\" stroke=\"black\" d=\"M192.23,-460.91C192.69,-450.2 193.19,-438.62 193.65,-427.78\"/>\n<polygon fill=\"black\" stroke=\"black\" points=\"197.15,-427.81 194.08,-417.67 190.16,-427.51 197.15,-427.81\"/>\n</g>\n<!-- 7 -->\n<g id=\"node8\" class=\"node\">\n<title>7</title>\n<path fill=\"#ffffff\" stroke=\"black\" d=\"M426,-425C426,-425 291,-425 291,-425 285,-425 279,-419 279,-413 279,-413 279,-354 279,-354 279,-348 285,-342 291,-342 291,-342 426,-342 426,-342 432,-342 438,-348 438,-354 438,-354 438,-413 438,-413 438,-419 432,-425 426,-425\"/>\n<text text-anchor=\"start\" x=\"287\" y=\"-409.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">petal length (cm) ‚â§ 4.95</text>\n<text text-anchor=\"start\" x=\"330.5\" y=\"-394.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">gini = 0.5</text>\n<text text-anchor=\"start\" x=\"321\" y=\"-379.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">samples = 8</text>\n<text text-anchor=\"start\" x=\"311.5\" y=\"-364.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">value = [0, 4, 4]</text>\n<text text-anchor=\"start\" x=\"306\" y=\"-349.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">class = versicolor</text>\n</g>\n<!-- 6&#45;&gt;7 -->\n<g id=\"edge7\" class=\"edge\">\n<title>6&#45;&gt;7</title>\n<path fill=\"none\" stroke=\"black\" d=\"M361.11,-460.91C360.83,-452.56 360.52,-443.67 360.23,-435.02\"/>\n<polygon fill=\"black\" stroke=\"black\" points=\"363.73,-434.9 359.89,-425.02 356.73,-435.13 363.73,-434.9\"/>\n</g>\n<!-- 14 -->\n<g id=\"node15\" class=\"node\">\n<title>14</title>\n<path fill=\"#853fe6\" stroke=\"black\" d=\"M603,-425C603,-425 468,-425 468,-425 462,-425 456,-419 456,-413 456,-413 456,-354 456,-354 456,-348 462,-342 468,-342 468,-342 603,-342 603,-342 609,-342 615,-348 615,-354 615,-354 615,-413 615,-413 615,-419 609,-425 603,-425\"/>\n<text text-anchor=\"start\" x=\"464\" y=\"-409.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">petal length (cm) ‚â§ 4.85</text>\n<text text-anchor=\"start\" x=\"500\" y=\"-394.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">gini = 0.056</text>\n<text text-anchor=\"start\" x=\"494.5\" y=\"-379.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">samples = 35</text>\n<text text-anchor=\"start\" x=\"485\" y=\"-364.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">value = [0, 1, 34]</text>\n<text text-anchor=\"start\" x=\"487\" y=\"-349.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">class = virginica</text>\n</g>\n<!-- 6&#45;&gt;14 -->\n<g id=\"edge14\" class=\"edge\">\n<title>6&#45;&gt;14</title>\n<path fill=\"none\" stroke=\"black\" d=\"M422.52,-460.91C436.88,-451.2 452.31,-440.76 467.02,-430.81\"/>\n<polygon fill=\"black\" stroke=\"black\" points=\"469.26,-433.52 475.58,-425.02 465.34,-427.72 469.26,-433.52\"/>\n</g>\n<!-- 8 -->\n<g id=\"node9\" class=\"node\">\n<title>8</title>\n<path fill=\"#39e581\" stroke=\"black\" d=\"M254,-298.5C254,-298.5 157,-298.5 157,-298.5 151,-298.5 145,-292.5 145,-286.5 145,-286.5 145,-242.5 145,-242.5 145,-236.5 151,-230.5 157,-230.5 157,-230.5 254,-230.5 254,-230.5 260,-230.5 266,-236.5 266,-242.5 266,-242.5 266,-286.5 266,-286.5 266,-292.5 260,-298.5 254,-298.5\"/>\n<text text-anchor=\"start\" x=\"177.5\" y=\"-283.3\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">gini = 0.0</text>\n<text text-anchor=\"start\" x=\"168\" y=\"-268.3\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">samples = 2</text>\n<text text-anchor=\"start\" x=\"158.5\" y=\"-253.3\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">value = [0, 2, 0]</text>\n<text text-anchor=\"start\" x=\"153\" y=\"-238.3\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">class = versicolor</text>\n</g>\n<!-- 7&#45;&gt;8 -->\n<g id=\"edge8\" class=\"edge\">\n<title>7&#45;&gt;8</title>\n<path fill=\"none\" stroke=\"black\" d=\"M305.42,-341.91C289.69,-329.88 272.5,-316.73 256.88,-304.79\"/>\n<polygon fill=\"black\" stroke=\"black\" points=\"258.94,-301.96 248.87,-298.67 254.69,-307.52 258.94,-301.96\"/>\n</g>\n<!-- 9 -->\n<g id=\"node10\" class=\"node\">\n<title>9</title>\n<path fill=\"#c09cf2\" stroke=\"black\" d=\"M426.5,-306C426.5,-306 296.5,-306 296.5,-306 290.5,-306 284.5,-300 284.5,-294 284.5,-294 284.5,-235 284.5,-235 284.5,-229 290.5,-223 296.5,-223 296.5,-223 426.5,-223 426.5,-223 432.5,-223 438.5,-229 438.5,-235 438.5,-235 438.5,-294 438.5,-294 438.5,-300 432.5,-306 426.5,-306\"/>\n<text text-anchor=\"start\" x=\"292.5\" y=\"-290.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">petal width (cm) ‚â§ 1.55</text>\n<text text-anchor=\"start\" x=\"326\" y=\"-275.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">gini = 0.444</text>\n<text text-anchor=\"start\" x=\"324\" y=\"-260.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">samples = 6</text>\n<text text-anchor=\"start\" x=\"314.5\" y=\"-245.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">value = [0, 2, 4]</text>\n<text text-anchor=\"start\" x=\"313\" y=\"-230.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">class = virginica</text>\n</g>\n<!-- 7&#45;&gt;9 -->\n<g id=\"edge9\" class=\"edge\">\n<title>7&#45;&gt;9</title>\n<path fill=\"none\" stroke=\"black\" d=\"M359.54,-341.91C359.75,-333.56 359.98,-324.67 360.2,-316.02\"/>\n<polygon fill=\"black\" stroke=\"black\" points=\"363.7,-316.11 360.46,-306.02 356.71,-315.93 363.7,-316.11\"/>\n</g>\n<!-- 10 -->\n<g id=\"node11\" class=\"node\">\n<title>10</title>\n<path fill=\"#8139e5\" stroke=\"black\" d=\"M256,-179.5C256,-179.5 167,-179.5 167,-179.5 161,-179.5 155,-173.5 155,-167.5 155,-167.5 155,-123.5 155,-123.5 155,-117.5 161,-111.5 167,-111.5 167,-111.5 256,-111.5 256,-111.5 262,-111.5 268,-117.5 268,-123.5 268,-123.5 268,-167.5 268,-167.5 268,-173.5 262,-179.5 256,-179.5\"/>\n<text text-anchor=\"start\" x=\"183.5\" y=\"-164.3\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">gini = 0.0</text>\n<text text-anchor=\"start\" x=\"174\" y=\"-149.3\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">samples = 3</text>\n<text text-anchor=\"start\" x=\"164.5\" y=\"-134.3\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">value = [0, 0, 3]</text>\n<text text-anchor=\"start\" x=\"163\" y=\"-119.3\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">class = virginica</text>\n</g>\n<!-- 9&#45;&gt;10 -->\n<g id=\"edge10\" class=\"edge\">\n<title>9&#45;&gt;10</title>\n<path fill=\"none\" stroke=\"black\" d=\"M309.46,-222.91C294.18,-210.99 277.49,-197.98 262.29,-186.12\"/>\n<polygon fill=\"black\" stroke=\"black\" points=\"264.06,-183.06 254.02,-179.67 259.75,-188.58 264.06,-183.06\"/>\n</g>\n<!-- 11 -->\n<g id=\"node12\" class=\"node\">\n<title>11</title>\n<path fill=\"#9cf2c0\" stroke=\"black\" d=\"M433,-187C433,-187 298,-187 298,-187 292,-187 286,-181 286,-175 286,-175 286,-116 286,-116 286,-110 292,-104 298,-104 298,-104 433,-104 433,-104 439,-104 445,-110 445,-116 445,-116 445,-175 445,-175 445,-181 439,-187 433,-187\"/>\n<text text-anchor=\"start\" x=\"294\" y=\"-171.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">petal length (cm) ‚â§ 5.45</text>\n<text text-anchor=\"start\" x=\"330\" y=\"-156.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">gini = 0.444</text>\n<text text-anchor=\"start\" x=\"328\" y=\"-141.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">samples = 3</text>\n<text text-anchor=\"start\" x=\"318.5\" y=\"-126.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">value = [0, 2, 1]</text>\n<text text-anchor=\"start\" x=\"313\" y=\"-111.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">class = versicolor</text>\n</g>\n<!-- 9&#45;&gt;11 -->\n<g id=\"edge11\" class=\"edge\">\n<title>9&#45;&gt;11</title>\n<path fill=\"none\" stroke=\"black\" d=\"M362.89,-222.91C363.17,-214.56 363.48,-205.67 363.77,-197.02\"/>\n<polygon fill=\"black\" stroke=\"black\" points=\"367.27,-197.13 364.11,-187.02 360.27,-196.9 367.27,-197.13\"/>\n</g>\n<!-- 12 -->\n<g id=\"node13\" class=\"node\">\n<title>12</title>\n<path fill=\"#39e581\" stroke=\"black\" d=\"M347,-68C347,-68 250,-68 250,-68 244,-68 238,-62 238,-56 238,-56 238,-12 238,-12 238,-6 244,0 250,0 250,0 347,0 347,0 353,0 359,-6 359,-12 359,-12 359,-56 359,-56 359,-62 353,-68 347,-68\"/>\n<text text-anchor=\"start\" x=\"270.5\" y=\"-52.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">gini = 0.0</text>\n<text text-anchor=\"start\" x=\"261\" y=\"-37.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">samples = 2</text>\n<text text-anchor=\"start\" x=\"251.5\" y=\"-22.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">value = [0, 2, 0]</text>\n<text text-anchor=\"start\" x=\"246\" y=\"-7.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">class = versicolor</text>\n</g>\n<!-- 11&#45;&gt;12 -->\n<g id=\"edge12\" class=\"edge\">\n<title>11&#45;&gt;12</title>\n<path fill=\"none\" stroke=\"black\" d=\"M340.55,-103.73C335.19,-94.97 329.52,-85.7 324.14,-76.91\"/>\n<polygon fill=\"black\" stroke=\"black\" points=\"327.08,-75 318.88,-68.3 321.11,-78.66 327.08,-75\"/>\n</g>\n<!-- 13 -->\n<g id=\"node14\" class=\"node\">\n<title>13</title>\n<path fill=\"#8139e5\" stroke=\"black\" d=\"M478,-68C478,-68 389,-68 389,-68 383,-68 377,-62 377,-56 377,-56 377,-12 377,-12 377,-6 383,0 389,0 389,0 478,0 478,0 484,0 490,-6 490,-12 490,-12 490,-56 490,-56 490,-62 484,-68 478,-68\"/>\n<text text-anchor=\"start\" x=\"405.5\" y=\"-52.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">gini = 0.0</text>\n<text text-anchor=\"start\" x=\"396\" y=\"-37.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">samples = 1</text>\n<text text-anchor=\"start\" x=\"386.5\" y=\"-22.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">value = [0, 0, 1]</text>\n<text text-anchor=\"start\" x=\"385\" y=\"-7.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">class = virginica</text>\n</g>\n<!-- 11&#45;&gt;13 -->\n<g id=\"edge13\" class=\"edge\">\n<title>11&#45;&gt;13</title>\n<path fill=\"none\" stroke=\"black\" d=\"M390.82,-103.73C396.26,-94.97 402.01,-85.7 407.48,-76.91\"/>\n<polygon fill=\"black\" stroke=\"black\" points=\"410.52,-78.64 412.82,-68.3 404.57,-74.95 410.52,-78.64\"/>\n</g>\n<!-- 15 -->\n<g id=\"node16\" class=\"node\">\n<title>15</title>\n<path fill=\"#c09cf2\" stroke=\"black\" d=\"M596,-306C596,-306 471,-306 471,-306 465,-306 459,-300 459,-294 459,-294 459,-235 459,-235 459,-229 465,-223 471,-223 471,-223 596,-223 596,-223 602,-223 608,-229 608,-235 608,-235 608,-294 608,-294 608,-300 602,-306 596,-306\"/>\n<text text-anchor=\"start\" x=\"467\" y=\"-290.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">sepal width (cm) ‚â§ 3.1</text>\n<text text-anchor=\"start\" x=\"498\" y=\"-275.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">gini = 0.444</text>\n<text text-anchor=\"start\" x=\"496\" y=\"-260.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">samples = 3</text>\n<text text-anchor=\"start\" x=\"486.5\" y=\"-245.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">value = [0, 1, 2]</text>\n<text text-anchor=\"start\" x=\"485\" y=\"-230.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">class = virginica</text>\n</g>\n<!-- 14&#45;&gt;15 -->\n<g id=\"edge15\" class=\"edge\">\n<title>14&#45;&gt;15</title>\n<path fill=\"none\" stroke=\"black\" d=\"M534.81,-341.91C534.66,-333.56 534.51,-324.67 534.36,-316.02\"/>\n<polygon fill=\"black\" stroke=\"black\" points=\"537.86,-315.96 534.19,-306.02 530.86,-316.08 537.86,-315.96\"/>\n</g>\n<!-- 18 -->\n<g id=\"node19\" class=\"node\">\n<title>18</title>\n<path fill=\"#8139e5\" stroke=\"black\" d=\"M731,-298.5C731,-298.5 638,-298.5 638,-298.5 632,-298.5 626,-292.5 626,-286.5 626,-286.5 626,-242.5 626,-242.5 626,-236.5 632,-230.5 638,-230.5 638,-230.5 731,-230.5 731,-230.5 737,-230.5 743,-236.5 743,-242.5 743,-242.5 743,-286.5 743,-286.5 743,-292.5 737,-298.5 731,-298.5\"/>\n<text text-anchor=\"start\" x=\"656.5\" y=\"-283.3\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">gini = 0.0</text>\n<text text-anchor=\"start\" x=\"643.5\" y=\"-268.3\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">samples = 32</text>\n<text text-anchor=\"start\" x=\"634\" y=\"-253.3\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">value = [0, 0, 32]</text>\n<text text-anchor=\"start\" x=\"636\" y=\"-238.3\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">class = virginica</text>\n</g>\n<!-- 14&#45;&gt;18 -->\n<g id=\"edge18\" class=\"edge\">\n<title>14&#45;&gt;18</title>\n<path fill=\"none\" stroke=\"black\" d=\"M587.19,-341.91C602.37,-329.99 618.95,-316.98 634.04,-305.12\"/>\n<polygon fill=\"black\" stroke=\"black\" points=\"636.56,-307.6 642.26,-298.67 632.24,-302.09 636.56,-307.6\"/>\n</g>\n<!-- 16 -->\n<g id=\"node17\" class=\"node\">\n<title>16</title>\n<path fill=\"#8139e5\" stroke=\"black\" d=\"M575,-179.5C575,-179.5 486,-179.5 486,-179.5 480,-179.5 474,-173.5 474,-167.5 474,-167.5 474,-123.5 474,-123.5 474,-117.5 480,-111.5 486,-111.5 486,-111.5 575,-111.5 575,-111.5 581,-111.5 587,-117.5 587,-123.5 587,-123.5 587,-167.5 587,-167.5 587,-173.5 581,-179.5 575,-179.5\"/>\n<text text-anchor=\"start\" x=\"502.5\" y=\"-164.3\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">gini = 0.0</text>\n<text text-anchor=\"start\" x=\"493\" y=\"-149.3\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">samples = 2</text>\n<text text-anchor=\"start\" x=\"483.5\" y=\"-134.3\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">value = [0, 0, 2]</text>\n<text text-anchor=\"start\" x=\"482\" y=\"-119.3\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">class = virginica</text>\n</g>\n<!-- 15&#45;&gt;16 -->\n<g id=\"edge16\" class=\"edge\">\n<title>15&#45;&gt;16</title>\n<path fill=\"none\" stroke=\"black\" d=\"M532.46,-222.91C532.18,-212.2 531.89,-200.62 531.61,-189.78\"/>\n<polygon fill=\"black\" stroke=\"black\" points=\"535.11,-189.57 531.35,-179.67 528.11,-189.75 535.11,-189.57\"/>\n</g>\n<!-- 17 -->\n<g id=\"node18\" class=\"node\">\n<title>17</title>\n<path fill=\"#39e581\" stroke=\"black\" d=\"M714,-179.5C714,-179.5 617,-179.5 617,-179.5 611,-179.5 605,-173.5 605,-167.5 605,-167.5 605,-123.5 605,-123.5 605,-117.5 611,-111.5 617,-111.5 617,-111.5 714,-111.5 714,-111.5 720,-111.5 726,-117.5 726,-123.5 726,-123.5 726,-167.5 726,-167.5 726,-173.5 720,-179.5 714,-179.5\"/>\n<text text-anchor=\"start\" x=\"637.5\" y=\"-164.3\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">gini = 0.0</text>\n<text text-anchor=\"start\" x=\"628\" y=\"-149.3\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">samples = 1</text>\n<text text-anchor=\"start\" x=\"618.5\" y=\"-134.3\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">value = [0, 1, 0]</text>\n<text text-anchor=\"start\" x=\"613\" y=\"-119.3\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">class = versicolor</text>\n</g>\n<!-- 15&#45;&gt;17 -->\n<g id=\"edge17\" class=\"edge\">\n<title>15&#45;&gt;17</title>\n<path fill=\"none\" stroke=\"black\" d=\"M579.3,-222.91C592.62,-211.1 607.15,-198.22 620.43,-186.45\"/>\n<polygon fill=\"black\" stroke=\"black\" points=\"622.92,-188.92 628.08,-179.67 618.28,-183.68 622.92,-188.92\"/>\n</g>\n</g>\n</svg>\n",
            "text/plain": [
              "<graphviz.sources.Source at 0x7d68e111fb90>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Visualization of Decision Tree with ccp_alpha = 0.01\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "image/svg+xml": "<?xml version=\"1.0\" encoding=\"UTF-8\" standalone=\"no\"?>\n<!DOCTYPE svg PUBLIC \"-//W3C//DTD SVG 1.1//EN\"\n \"http://www.w3.org/Graphics/SVG/1.1/DTD/svg11.dtd\">\n<!-- Generated by graphviz version 2.43.0 (0)\n -->\n<!-- Title: Tree Pages: 1 -->\n<svg width=\"645pt\" height=\"790pt\"\n viewBox=\"0.00 0.00 645.00 790.00\" xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">\n<g id=\"graph0\" class=\"graph\" transform=\"scale(1 1) rotate(0) translate(4 786)\">\n<title>Tree</title>\n<polygon fill=\"white\" stroke=\"transparent\" points=\"-4,4 -4,-786 641,-786 641,4 -4,4\"/>\n<!-- 0 -->\n<g id=\"node1\" class=\"node\">\n<title>0</title>\n<path fill=\"#fdfffd\" stroke=\"black\" d=\"M266,-782C266,-782 131,-782 131,-782 125,-782 119,-776 119,-770 119,-770 119,-711 119,-711 119,-705 125,-699 131,-699 131,-699 266,-699 266,-699 272,-699 278,-705 278,-711 278,-711 278,-770 278,-770 278,-776 272,-782 266,-782\"/>\n<text text-anchor=\"start\" x=\"127\" y=\"-766.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">petal length (cm) ‚â§ 2.45</text>\n<text text-anchor=\"start\" x=\"163\" y=\"-751.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">gini = 0.667</text>\n<text text-anchor=\"start\" x=\"153.5\" y=\"-736.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">samples = 120</text>\n<text text-anchor=\"start\" x=\"140.5\" y=\"-721.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">value = [40, 41, 39]</text>\n<text text-anchor=\"start\" x=\"146\" y=\"-706.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">class = versicolor</text>\n</g>\n<!-- 1 -->\n<g id=\"node2\" class=\"node\">\n<title>1</title>\n<path fill=\"#e58139\" stroke=\"black\" d=\"M167,-655.5C167,-655.5 74,-655.5 74,-655.5 68,-655.5 62,-649.5 62,-643.5 62,-643.5 62,-599.5 62,-599.5 62,-593.5 68,-587.5 74,-587.5 74,-587.5 167,-587.5 167,-587.5 173,-587.5 179,-593.5 179,-599.5 179,-599.5 179,-643.5 179,-643.5 179,-649.5 173,-655.5 167,-655.5\"/>\n<text text-anchor=\"start\" x=\"92.5\" y=\"-640.3\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">gini = 0.0</text>\n<text text-anchor=\"start\" x=\"79.5\" y=\"-625.3\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">samples = 40</text>\n<text text-anchor=\"start\" x=\"70\" y=\"-610.3\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">value = [40, 0, 0]</text>\n<text text-anchor=\"start\" x=\"77\" y=\"-595.3\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">class = setosa</text>\n</g>\n<!-- 0&#45;&gt;1 -->\n<g id=\"edge1\" class=\"edge\">\n<title>0&#45;&gt;1</title>\n<path fill=\"none\" stroke=\"black\" d=\"M171.44,-698.91C163.93,-687.65 155.78,-675.42 148.24,-664.11\"/>\n<polygon fill=\"black\" stroke=\"black\" points=\"151.07,-662.05 142.61,-655.67 145.25,-665.93 151.07,-662.05\"/>\n<text text-anchor=\"middle\" x=\"137.71\" y=\"-676.48\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">True</text>\n</g>\n<!-- 2 -->\n<g id=\"node3\" class=\"node\">\n<title>2</title>\n<path fill=\"#f5fef9\" stroke=\"black\" d=\"M344,-663C344,-663 209,-663 209,-663 203,-663 197,-657 197,-651 197,-651 197,-592 197,-592 197,-586 203,-580 209,-580 209,-580 344,-580 344,-580 350,-580 356,-586 356,-592 356,-592 356,-651 356,-651 356,-657 350,-663 344,-663\"/>\n<text text-anchor=\"start\" x=\"205\" y=\"-647.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">petal length (cm) ‚â§ 4.75</text>\n<text text-anchor=\"start\" x=\"248.5\" y=\"-632.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">gini = 0.5</text>\n<text text-anchor=\"start\" x=\"235.5\" y=\"-617.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">samples = 80</text>\n<text text-anchor=\"start\" x=\"222\" y=\"-602.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">value = [0, 41, 39]</text>\n<text text-anchor=\"start\" x=\"224\" y=\"-587.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">class = versicolor</text>\n</g>\n<!-- 0&#45;&gt;2 -->\n<g id=\"edge2\" class=\"edge\">\n<title>0&#45;&gt;2</title>\n<path fill=\"none\" stroke=\"black\" d=\"M225.56,-698.91C231.43,-690.1 237.7,-680.7 243.76,-671.61\"/>\n<polygon fill=\"black\" stroke=\"black\" points=\"246.85,-673.28 249.49,-663.02 241.03,-669.4 246.85,-673.28\"/>\n<text text-anchor=\"middle\" x=\"254.39\" y=\"-683.84\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">False</text>\n</g>\n<!-- 3 -->\n<g id=\"node4\" class=\"node\">\n<title>3</title>\n<path fill=\"#3ee684\" stroke=\"black\" d=\"M255.5,-544C255.5,-544 125.5,-544 125.5,-544 119.5,-544 113.5,-538 113.5,-532 113.5,-532 113.5,-473 113.5,-473 113.5,-467 119.5,-461 125.5,-461 125.5,-461 255.5,-461 255.5,-461 261.5,-461 267.5,-467 267.5,-473 267.5,-473 267.5,-532 267.5,-532 267.5,-538 261.5,-544 255.5,-544\"/>\n<text text-anchor=\"start\" x=\"121.5\" y=\"-528.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">petal width (cm) ‚â§ 1.65</text>\n<text text-anchor=\"start\" x=\"155\" y=\"-513.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">gini = 0.053</text>\n<text text-anchor=\"start\" x=\"149.5\" y=\"-498.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">samples = 37</text>\n<text text-anchor=\"start\" x=\"140\" y=\"-483.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">value = [0, 36, 1]</text>\n<text text-anchor=\"start\" x=\"138\" y=\"-468.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">class = versicolor</text>\n</g>\n<!-- 2&#45;&gt;3 -->\n<g id=\"edge3\" class=\"edge\">\n<title>2&#45;&gt;3</title>\n<path fill=\"none\" stroke=\"black\" d=\"M246.66,-579.91C240.13,-571.01 233.14,-561.51 226.39,-552.33\"/>\n<polygon fill=\"black\" stroke=\"black\" points=\"229.03,-550.01 220.28,-544.02 223.39,-554.15 229.03,-550.01\"/>\n</g>\n<!-- 6 -->\n<g id=\"node7\" class=\"node\">\n<title>6</title>\n<path fill=\"#9253e8\" stroke=\"black\" d=\"M427.5,-544C427.5,-544 297.5,-544 297.5,-544 291.5,-544 285.5,-538 285.5,-532 285.5,-532 285.5,-473 285.5,-473 285.5,-467 291.5,-461 297.5,-461 297.5,-461 427.5,-461 427.5,-461 433.5,-461 439.5,-467 439.5,-473 439.5,-473 439.5,-532 439.5,-532 439.5,-538 433.5,-544 427.5,-544\"/>\n<text text-anchor=\"start\" x=\"293.5\" y=\"-528.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">petal width (cm) ‚â§ 1.75</text>\n<text text-anchor=\"start\" x=\"327\" y=\"-513.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">gini = 0.206</text>\n<text text-anchor=\"start\" x=\"321.5\" y=\"-498.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">samples = 43</text>\n<text text-anchor=\"start\" x=\"312\" y=\"-483.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">value = [0, 5, 38]</text>\n<text text-anchor=\"start\" x=\"314\" y=\"-468.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">class = virginica</text>\n</g>\n<!-- 2&#45;&gt;6 -->\n<g id=\"edge6\" class=\"edge\">\n<title>2&#45;&gt;6</title>\n<path fill=\"none\" stroke=\"black\" d=\"M306.34,-579.91C312.87,-571.01 319.86,-561.51 326.61,-552.33\"/>\n<polygon fill=\"black\" stroke=\"black\" points=\"329.61,-554.15 332.72,-544.02 323.97,-550.01 329.61,-554.15\"/>\n</g>\n<!-- 4 -->\n<g id=\"node5\" class=\"node\">\n<title>4</title>\n<path fill=\"#39e581\" stroke=\"black\" d=\"M109,-417.5C109,-417.5 12,-417.5 12,-417.5 6,-417.5 0,-411.5 0,-405.5 0,-405.5 0,-361.5 0,-361.5 0,-355.5 6,-349.5 12,-349.5 12,-349.5 109,-349.5 109,-349.5 115,-349.5 121,-355.5 121,-361.5 121,-361.5 121,-405.5 121,-405.5 121,-411.5 115,-417.5 109,-417.5\"/>\n<text text-anchor=\"start\" x=\"32.5\" y=\"-402.3\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">gini = 0.0</text>\n<text text-anchor=\"start\" x=\"19.5\" y=\"-387.3\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">samples = 36</text>\n<text text-anchor=\"start\" x=\"10\" y=\"-372.3\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">value = [0, 36, 0]</text>\n<text text-anchor=\"start\" x=\"8\" y=\"-357.3\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">class = versicolor</text>\n</g>\n<!-- 3&#45;&gt;4 -->\n<g id=\"edge4\" class=\"edge\">\n<title>3&#45;&gt;4</title>\n<path fill=\"none\" stroke=\"black\" d=\"M145.4,-460.91C132.28,-449.1 117.96,-436.22 104.89,-424.45\"/>\n<polygon fill=\"black\" stroke=\"black\" points=\"107.13,-421.76 97.35,-417.67 102.44,-426.96 107.13,-421.76\"/>\n</g>\n<!-- 5 -->\n<g id=\"node6\" class=\"node\">\n<title>5</title>\n<path fill=\"#8139e5\" stroke=\"black\" d=\"M240,-417.5C240,-417.5 151,-417.5 151,-417.5 145,-417.5 139,-411.5 139,-405.5 139,-405.5 139,-361.5 139,-361.5 139,-355.5 145,-349.5 151,-349.5 151,-349.5 240,-349.5 240,-349.5 246,-349.5 252,-355.5 252,-361.5 252,-361.5 252,-405.5 252,-405.5 252,-411.5 246,-417.5 240,-417.5\"/>\n<text text-anchor=\"start\" x=\"167.5\" y=\"-402.3\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">gini = 0.0</text>\n<text text-anchor=\"start\" x=\"158\" y=\"-387.3\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">samples = 1</text>\n<text text-anchor=\"start\" x=\"148.5\" y=\"-372.3\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">value = [0, 0, 1]</text>\n<text text-anchor=\"start\" x=\"147\" y=\"-357.3\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">class = virginica</text>\n</g>\n<!-- 3&#45;&gt;5 -->\n<g id=\"edge5\" class=\"edge\">\n<title>3&#45;&gt;5</title>\n<path fill=\"none\" stroke=\"black\" d=\"M192.23,-460.91C192.69,-450.2 193.19,-438.62 193.65,-427.78\"/>\n<polygon fill=\"black\" stroke=\"black\" points=\"197.15,-427.81 194.08,-417.67 190.16,-427.51 197.15,-427.81\"/>\n</g>\n<!-- 7 -->\n<g id=\"node8\" class=\"node\">\n<title>7</title>\n<path fill=\"#ffffff\" stroke=\"black\" d=\"M426,-425C426,-425 291,-425 291,-425 285,-425 279,-419 279,-413 279,-413 279,-354 279,-354 279,-348 285,-342 291,-342 291,-342 426,-342 426,-342 432,-342 438,-348 438,-354 438,-354 438,-413 438,-413 438,-419 432,-425 426,-425\"/>\n<text text-anchor=\"start\" x=\"287\" y=\"-409.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">petal length (cm) ‚â§ 4.95</text>\n<text text-anchor=\"start\" x=\"330.5\" y=\"-394.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">gini = 0.5</text>\n<text text-anchor=\"start\" x=\"321\" y=\"-379.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">samples = 8</text>\n<text text-anchor=\"start\" x=\"311.5\" y=\"-364.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">value = [0, 4, 4]</text>\n<text text-anchor=\"start\" x=\"306\" y=\"-349.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">class = versicolor</text>\n</g>\n<!-- 6&#45;&gt;7 -->\n<g id=\"edge7\" class=\"edge\">\n<title>6&#45;&gt;7</title>\n<path fill=\"none\" stroke=\"black\" d=\"M361.11,-460.91C360.83,-452.56 360.52,-443.67 360.23,-435.02\"/>\n<polygon fill=\"black\" stroke=\"black\" points=\"363.73,-434.9 359.89,-425.02 356.73,-435.13 363.73,-434.9\"/>\n</g>\n<!-- 14 -->\n<g id=\"node15\" class=\"node\">\n<title>14</title>\n<path fill=\"#853fe6\" stroke=\"black\" d=\"M561,-417.5C561,-417.5 468,-417.5 468,-417.5 462,-417.5 456,-411.5 456,-405.5 456,-405.5 456,-361.5 456,-361.5 456,-355.5 462,-349.5 468,-349.5 468,-349.5 561,-349.5 561,-349.5 567,-349.5 573,-355.5 573,-361.5 573,-361.5 573,-405.5 573,-405.5 573,-411.5 567,-417.5 561,-417.5\"/>\n<text text-anchor=\"start\" x=\"479\" y=\"-402.3\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">gini = 0.056</text>\n<text text-anchor=\"start\" x=\"473.5\" y=\"-387.3\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">samples = 35</text>\n<text text-anchor=\"start\" x=\"464\" y=\"-372.3\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">value = [0, 1, 34]</text>\n<text text-anchor=\"start\" x=\"466\" y=\"-357.3\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">class = virginica</text>\n</g>\n<!-- 6&#45;&gt;14 -->\n<g id=\"edge14\" class=\"edge\">\n<title>6&#45;&gt;14</title>\n<path fill=\"none\" stroke=\"black\" d=\"M415.24,-460.91C430.86,-448.88 447.94,-435.73 463.46,-423.79\"/>\n<polygon fill=\"black\" stroke=\"black\" points=\"465.62,-426.54 471.41,-417.67 461.35,-420.99 465.62,-426.54\"/>\n</g>\n<!-- 8 -->\n<g id=\"node9\" class=\"node\">\n<title>8</title>\n<path fill=\"#39e581\" stroke=\"black\" d=\"M329,-298.5C329,-298.5 232,-298.5 232,-298.5 226,-298.5 220,-292.5 220,-286.5 220,-286.5 220,-242.5 220,-242.5 220,-236.5 226,-230.5 232,-230.5 232,-230.5 329,-230.5 329,-230.5 335,-230.5 341,-236.5 341,-242.5 341,-242.5 341,-286.5 341,-286.5 341,-292.5 335,-298.5 329,-298.5\"/>\n<text text-anchor=\"start\" x=\"252.5\" y=\"-283.3\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">gini = 0.0</text>\n<text text-anchor=\"start\" x=\"243\" y=\"-268.3\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">samples = 2</text>\n<text text-anchor=\"start\" x=\"233.5\" y=\"-253.3\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">value = [0, 2, 0]</text>\n<text text-anchor=\"start\" x=\"228\" y=\"-238.3\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">class = versicolor</text>\n</g>\n<!-- 7&#45;&gt;8 -->\n<g id=\"edge8\" class=\"edge\">\n<title>7&#45;&gt;8</title>\n<path fill=\"none\" stroke=\"black\" d=\"M331.44,-341.91C323.93,-330.65 315.78,-318.42 308.24,-307.11\"/>\n<polygon fill=\"black\" stroke=\"black\" points=\"311.07,-305.05 302.61,-298.67 305.25,-308.93 311.07,-305.05\"/>\n</g>\n<!-- 9 -->\n<g id=\"node10\" class=\"node\">\n<title>9</title>\n<path fill=\"#c09cf2\" stroke=\"black\" d=\"M501.5,-306C501.5,-306 371.5,-306 371.5,-306 365.5,-306 359.5,-300 359.5,-294 359.5,-294 359.5,-235 359.5,-235 359.5,-229 365.5,-223 371.5,-223 371.5,-223 501.5,-223 501.5,-223 507.5,-223 513.5,-229 513.5,-235 513.5,-235 513.5,-294 513.5,-294 513.5,-300 507.5,-306 501.5,-306\"/>\n<text text-anchor=\"start\" x=\"367.5\" y=\"-290.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">petal width (cm) ‚â§ 1.55</text>\n<text text-anchor=\"start\" x=\"401\" y=\"-275.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">gini = 0.444</text>\n<text text-anchor=\"start\" x=\"399\" y=\"-260.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">samples = 6</text>\n<text text-anchor=\"start\" x=\"389.5\" y=\"-245.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">value = [0, 2, 4]</text>\n<text text-anchor=\"start\" x=\"388\" y=\"-230.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">class = virginica</text>\n</g>\n<!-- 7&#45;&gt;9 -->\n<g id=\"edge9\" class=\"edge\">\n<title>7&#45;&gt;9</title>\n<path fill=\"none\" stroke=\"black\" d=\"M385.56,-341.91C391.43,-333.1 397.7,-323.7 403.76,-314.61\"/>\n<polygon fill=\"black\" stroke=\"black\" points=\"406.85,-316.28 409.49,-306.02 401.03,-312.4 406.85,-316.28\"/>\n</g>\n<!-- 10 -->\n<g id=\"node11\" class=\"node\">\n<title>10</title>\n<path fill=\"#8139e5\" stroke=\"black\" d=\"M404,-179.5C404,-179.5 315,-179.5 315,-179.5 309,-179.5 303,-173.5 303,-167.5 303,-167.5 303,-123.5 303,-123.5 303,-117.5 309,-111.5 315,-111.5 315,-111.5 404,-111.5 404,-111.5 410,-111.5 416,-117.5 416,-123.5 416,-123.5 416,-167.5 416,-167.5 416,-173.5 410,-179.5 404,-179.5\"/>\n<text text-anchor=\"start\" x=\"331.5\" y=\"-164.3\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">gini = 0.0</text>\n<text text-anchor=\"start\" x=\"322\" y=\"-149.3\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">samples = 3</text>\n<text text-anchor=\"start\" x=\"312.5\" y=\"-134.3\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">value = [0, 0, 3]</text>\n<text text-anchor=\"start\" x=\"311\" y=\"-119.3\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">class = virginica</text>\n</g>\n<!-- 9&#45;&gt;10 -->\n<g id=\"edge10\" class=\"edge\">\n<title>9&#45;&gt;10</title>\n<path fill=\"none\" stroke=\"black\" d=\"M409.79,-222.91C402.38,-211.65 394.33,-199.42 386.88,-188.11\"/>\n<polygon fill=\"black\" stroke=\"black\" points=\"389.75,-186.1 381.33,-179.67 383.9,-189.94 389.75,-186.1\"/>\n</g>\n<!-- 11 -->\n<g id=\"node12\" class=\"node\">\n<title>11</title>\n<path fill=\"#9cf2c0\" stroke=\"black\" d=\"M581,-187C581,-187 446,-187 446,-187 440,-187 434,-181 434,-175 434,-175 434,-116 434,-116 434,-110 440,-104 446,-104 446,-104 581,-104 581,-104 587,-104 593,-110 593,-116 593,-116 593,-175 593,-175 593,-181 587,-187 581,-187\"/>\n<text text-anchor=\"start\" x=\"442\" y=\"-171.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">petal length (cm) ‚â§ 5.45</text>\n<text text-anchor=\"start\" x=\"478\" y=\"-156.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">gini = 0.444</text>\n<text text-anchor=\"start\" x=\"476\" y=\"-141.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">samples = 3</text>\n<text text-anchor=\"start\" x=\"466.5\" y=\"-126.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">value = [0, 2, 1]</text>\n<text text-anchor=\"start\" x=\"461\" y=\"-111.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">class = versicolor</text>\n</g>\n<!-- 9&#45;&gt;11 -->\n<g id=\"edge11\" class=\"edge\">\n<title>9&#45;&gt;11</title>\n<path fill=\"none\" stroke=\"black\" d=\"M463.21,-222.91C469.01,-214.1 475.2,-204.7 481.18,-195.61\"/>\n<polygon fill=\"black\" stroke=\"black\" points=\"484.26,-197.3 486.83,-187.02 478.41,-193.45 484.26,-197.3\"/>\n</g>\n<!-- 12 -->\n<g id=\"node13\" class=\"node\">\n<title>12</title>\n<path fill=\"#39e581\" stroke=\"black\" d=\"M494,-68C494,-68 397,-68 397,-68 391,-68 385,-62 385,-56 385,-56 385,-12 385,-12 385,-6 391,0 397,0 397,0 494,0 494,0 500,0 506,-6 506,-12 506,-12 506,-56 506,-56 506,-62 500,-68 494,-68\"/>\n<text text-anchor=\"start\" x=\"417.5\" y=\"-52.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">gini = 0.0</text>\n<text text-anchor=\"start\" x=\"408\" y=\"-37.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">samples = 2</text>\n<text text-anchor=\"start\" x=\"398.5\" y=\"-22.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">value = [0, 2, 0]</text>\n<text text-anchor=\"start\" x=\"393\" y=\"-7.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">class = versicolor</text>\n</g>\n<!-- 11&#45;&gt;12 -->\n<g id=\"edge12\" class=\"edge\">\n<title>11&#45;&gt;12</title>\n<path fill=\"none\" stroke=\"black\" d=\"M488.18,-103.73C482.74,-94.97 476.99,-85.7 471.52,-76.91\"/>\n<polygon fill=\"black\" stroke=\"black\" points=\"474.43,-74.95 466.18,-68.3 468.48,-78.64 474.43,-74.95\"/>\n</g>\n<!-- 13 -->\n<g id=\"node14\" class=\"node\">\n<title>13</title>\n<path fill=\"#8139e5\" stroke=\"black\" d=\"M625,-68C625,-68 536,-68 536,-68 530,-68 524,-62 524,-56 524,-56 524,-12 524,-12 524,-6 530,0 536,0 536,0 625,0 625,0 631,0 637,-6 637,-12 637,-12 637,-56 637,-56 637,-62 631,-68 625,-68\"/>\n<text text-anchor=\"start\" x=\"552.5\" y=\"-52.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">gini = 0.0</text>\n<text text-anchor=\"start\" x=\"543\" y=\"-37.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">samples = 1</text>\n<text text-anchor=\"start\" x=\"533.5\" y=\"-22.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">value = [0, 0, 1]</text>\n<text text-anchor=\"start\" x=\"532\" y=\"-7.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">class = virginica</text>\n</g>\n<!-- 11&#45;&gt;13 -->\n<g id=\"edge13\" class=\"edge\">\n<title>11&#45;&gt;13</title>\n<path fill=\"none\" stroke=\"black\" d=\"M538.45,-103.73C543.81,-94.97 549.48,-85.7 554.86,-76.91\"/>\n<polygon fill=\"black\" stroke=\"black\" points=\"557.89,-78.66 560.12,-68.3 551.92,-75 557.89,-78.66\"/>\n</g>\n</g>\n</svg>\n",
            "text/plain": [
              "<graphviz.sources.Source at 0x7d68c19e7150>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Visualization of Decision Tree with ccp_alpha = 0.05\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "image/svg+xml": "<?xml version=\"1.0\" encoding=\"UTF-8\" standalone=\"no\"?>\n<!DOCTYPE svg PUBLIC \"-//W3C//DTD SVG 1.1//EN\"\n \"http://www.w3.org/Graphics/SVG/1.1/DTD/svg11.dtd\">\n<!-- Generated by graphviz version 2.43.0 (0)\n -->\n<!-- Title: Tree Pages: 1 -->\n<svg width=\"349pt\" height=\"314pt\"\n viewBox=\"0.00 0.00 349.00 314.00\" xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">\n<g id=\"graph0\" class=\"graph\" transform=\"scale(1 1) rotate(0) translate(4 310)\">\n<title>Tree</title>\n<polygon fill=\"white\" stroke=\"transparent\" points=\"-4,4 -4,-310 345,-310 345,4 -4,4\"/>\n<!-- 0 -->\n<g id=\"node1\" class=\"node\">\n<title>0</title>\n<path fill=\"#fdfffd\" stroke=\"black\" d=\"M204,-306C204,-306 69,-306 69,-306 63,-306 57,-300 57,-294 57,-294 57,-235 57,-235 57,-229 63,-223 69,-223 69,-223 204,-223 204,-223 210,-223 216,-229 216,-235 216,-235 216,-294 216,-294 216,-300 210,-306 204,-306\"/>\n<text text-anchor=\"start\" x=\"65\" y=\"-290.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">petal length (cm) ‚â§ 2.45</text>\n<text text-anchor=\"start\" x=\"101\" y=\"-275.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">gini = 0.667</text>\n<text text-anchor=\"start\" x=\"91.5\" y=\"-260.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">samples = 120</text>\n<text text-anchor=\"start\" x=\"78.5\" y=\"-245.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">value = [40, 41, 39]</text>\n<text text-anchor=\"start\" x=\"84\" y=\"-230.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">class = versicolor</text>\n</g>\n<!-- 1 -->\n<g id=\"node2\" class=\"node\">\n<title>1</title>\n<path fill=\"#e58139\" stroke=\"black\" d=\"M105,-179.5C105,-179.5 12,-179.5 12,-179.5 6,-179.5 0,-173.5 0,-167.5 0,-167.5 0,-123.5 0,-123.5 0,-117.5 6,-111.5 12,-111.5 12,-111.5 105,-111.5 105,-111.5 111,-111.5 117,-117.5 117,-123.5 117,-123.5 117,-167.5 117,-167.5 117,-173.5 111,-179.5 105,-179.5\"/>\n<text text-anchor=\"start\" x=\"30.5\" y=\"-164.3\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">gini = 0.0</text>\n<text text-anchor=\"start\" x=\"17.5\" y=\"-149.3\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">samples = 40</text>\n<text text-anchor=\"start\" x=\"8\" y=\"-134.3\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">value = [40, 0, 0]</text>\n<text text-anchor=\"start\" x=\"15\" y=\"-119.3\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">class = setosa</text>\n</g>\n<!-- 0&#45;&gt;1 -->\n<g id=\"edge1\" class=\"edge\">\n<title>0&#45;&gt;1</title>\n<path fill=\"none\" stroke=\"black\" d=\"M109.44,-222.91C101.93,-211.65 93.78,-199.42 86.24,-188.11\"/>\n<polygon fill=\"black\" stroke=\"black\" points=\"89.07,-186.05 80.61,-179.67 83.25,-189.93 89.07,-186.05\"/>\n<text text-anchor=\"middle\" x=\"75.71\" y=\"-200.48\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">True</text>\n</g>\n<!-- 2 -->\n<g id=\"node3\" class=\"node\">\n<title>2</title>\n<path fill=\"#f5fef9\" stroke=\"black\" d=\"M282,-187C282,-187 147,-187 147,-187 141,-187 135,-181 135,-175 135,-175 135,-116 135,-116 135,-110 141,-104 147,-104 147,-104 282,-104 282,-104 288,-104 294,-110 294,-116 294,-116 294,-175 294,-175 294,-181 288,-187 282,-187\"/>\n<text text-anchor=\"start\" x=\"143\" y=\"-171.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">petal length (cm) ‚â§ 4.75</text>\n<text text-anchor=\"start\" x=\"186.5\" y=\"-156.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">gini = 0.5</text>\n<text text-anchor=\"start\" x=\"173.5\" y=\"-141.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">samples = 80</text>\n<text text-anchor=\"start\" x=\"160\" y=\"-126.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">value = [0, 41, 39]</text>\n<text text-anchor=\"start\" x=\"162\" y=\"-111.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">class = versicolor</text>\n</g>\n<!-- 0&#45;&gt;2 -->\n<g id=\"edge2\" class=\"edge\">\n<title>0&#45;&gt;2</title>\n<path fill=\"none\" stroke=\"black\" d=\"M163.56,-222.91C169.43,-214.1 175.7,-204.7 181.76,-195.61\"/>\n<polygon fill=\"black\" stroke=\"black\" points=\"184.85,-197.28 187.49,-187.02 179.03,-193.4 184.85,-197.28\"/>\n<text text-anchor=\"middle\" x=\"192.39\" y=\"-207.84\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">False</text>\n</g>\n<!-- 3 -->\n<g id=\"node4\" class=\"node\">\n<title>3</title>\n<path fill=\"#3ee684\" stroke=\"black\" d=\"M194,-68C194,-68 97,-68 97,-68 91,-68 85,-62 85,-56 85,-56 85,-12 85,-12 85,-6 91,0 97,0 97,0 194,0 194,0 200,0 206,-6 206,-12 206,-12 206,-56 206,-56 206,-62 200,-68 194,-68\"/>\n<text text-anchor=\"start\" x=\"110\" y=\"-52.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">gini = 0.053</text>\n<text text-anchor=\"start\" x=\"104.5\" y=\"-37.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">samples = 37</text>\n<text text-anchor=\"start\" x=\"95\" y=\"-22.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">value = [0, 36, 1]</text>\n<text text-anchor=\"start\" x=\"93\" y=\"-7.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">class = versicolor</text>\n</g>\n<!-- 2&#45;&gt;3 -->\n<g id=\"edge3\" class=\"edge\">\n<title>2&#45;&gt;3</title>\n<path fill=\"none\" stroke=\"black\" d=\"M188.81,-103.73C183.29,-94.97 177.45,-85.7 171.91,-76.91\"/>\n<polygon fill=\"black\" stroke=\"black\" points=\"174.78,-74.89 166.48,-68.3 168.85,-78.63 174.78,-74.89\"/>\n</g>\n<!-- 4 -->\n<g id=\"node5\" class=\"node\">\n<title>4</title>\n<path fill=\"#9253e8\" stroke=\"black\" d=\"M329,-68C329,-68 236,-68 236,-68 230,-68 224,-62 224,-56 224,-56 224,-12 224,-12 224,-6 230,0 236,0 236,0 329,0 329,0 335,0 341,-6 341,-12 341,-12 341,-56 341,-56 341,-62 335,-68 329,-68\"/>\n<text text-anchor=\"start\" x=\"247\" y=\"-52.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">gini = 0.206</text>\n<text text-anchor=\"start\" x=\"241.5\" y=\"-37.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">samples = 43</text>\n<text text-anchor=\"start\" x=\"232\" y=\"-22.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">value = [0, 5, 38]</text>\n<text text-anchor=\"start\" x=\"234\" y=\"-7.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">class = virginica</text>\n</g>\n<!-- 2&#45;&gt;4 -->\n<g id=\"edge4\" class=\"edge\">\n<title>2&#45;&gt;4</title>\n<path fill=\"none\" stroke=\"black\" d=\"M239.82,-103.73C245.26,-94.97 251.01,-85.7 256.48,-76.91\"/>\n<polygon fill=\"black\" stroke=\"black\" points=\"259.52,-78.64 261.82,-68.3 253.57,-74.95 259.52,-78.64\"/>\n</g>\n</g>\n</svg>\n",
            "text/plain": [
              "<graphviz.sources.Source at 0x7d68c198c950>"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "28. Write a Python program to train a Decision Tree Classifier and evaluate its performance using Precision,  Recall, and F1-Score.\n",
        "\n",
        "ans. Here‚Äôs a **Python program** to train a **Decision Tree Classifier** and evaluate its performance using **Precision, Recall, and F1-Score**. üöÄ  \n",
        "\n",
        "---\n",
        "\n",
        "### **üìå Steps in the Code:**  \n",
        "1Ô∏è‚É£ Load the **Iris dataset**.  \n",
        "2Ô∏è‚É£ Split into **training (80%) and testing (20%) sets**.  \n",
        "3Ô∏è‚É£ Train a **Decision Tree Classifier**.  \n",
        "4Ô∏è‚É£ Make **predictions** on the test data.  \n",
        "5Ô∏è‚É£ Evaluate using **Precision, Recall, and F1-Score**.  \n",
        "\n",
        "---\n",
        "\n",
        "### **‚úÖ Python Code**\n",
        "```python\n",
        "# Import necessary libraries\n",
        "from sklearn.datasets import load_iris\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.metrics import classification_report\n",
        "\n",
        "# Step 1: Load the Iris dataset\n",
        "iris = load_iris()\n",
        "X = iris.data  # Features\n",
        "y = iris.target  # Target labels\n",
        "\n",
        "# Step 2: Split dataset into Training (80%) and Testing (20%) sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Step 3: Train Decision Tree Classifier\n",
        "clf = DecisionTreeClassifier(random_state=42)\n",
        "clf.fit(X_train, y_train)\n",
        "\n",
        "# Step 4: Make Predictions\n",
        "y_pred = clf.predict(X_test)\n",
        "\n",
        "# Step 5: Evaluate Performance using Precision, Recall, and F1-Score\n",
        "report = classification_report(y_test, y_pred, target_names=iris.target_names)\n",
        "\n",
        "# Print Evaluation Metrics\n",
        "print(\"Classification Report:\\n\")\n",
        "print(report)\n",
        "```\n",
        "\n",
        "---\n",
        "\n",
        "### **üîπ Example Output:**\n",
        "```\n",
        "Classification Report:\n",
        "\n",
        "              precision    recall  f1-score   support\n",
        "\n",
        "      setosa       1.00      1.00      1.00         8\n",
        "  versicolor       1.00      1.00      1.00        10\n",
        "   virginica       1.00      1.00      1.00        12\n",
        "\n",
        "    accuracy                           1.00        30\n",
        "   macro avg       1.00      1.00      1.00        30\n",
        "weighted avg       1.00      1.00      1.00        30\n",
        "```\n",
        "\n",
        "---\n",
        "\n",
        "### **üìå Explanation:**\n",
        "‚úÖ **Precision** ‚Üí Measures how many of the predicted positives were actually correct.  \n",
        "‚úÖ **Recall** ‚Üí Measures how many actual positives were correctly predicted.  \n",
        "‚úÖ **F1-Score** ‚Üí Harmonic mean of Precision & Recall (good balance indicator).  \n",
        "‚úÖ **`classification_report()`** ‚Üí Automatically calculates **all three metrics** for each class.  "
      ],
      "metadata": {
        "id": "TH3McCETfLNm"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "29.  Write a Python program to train a Decision Tree Classifier and visualize the confusion matrix using seaborn"
      ],
      "metadata": {
        "id": "l8ZxvDrCfZMM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#ans\n",
        "\n",
        "# Import necessary libraries\n",
        "import numpy as np\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.datasets import load_iris\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.metrics import confusion_matrix, classification_report\n",
        "\n",
        "# Step 1: Load the Iris dataset\n",
        "iris = load_iris()\n",
        "X = iris.data  # Features\n",
        "y = iris.target  # Target labels\n",
        "\n",
        "# Step 2: Split dataset into Training (80%) and Testing (20%) sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Step 3: Train Decision Tree Classifier\n",
        "clf = DecisionTreeClassifier(random_state=42)\n",
        "clf.fit(X_train, y_train)\n",
        "\n",
        "# Step 4: Make Predictions\n",
        "y_pred = clf.predict(X_test)\n",
        "\n",
        "# Step 5: Compute Confusion Matrix\n",
        "cm = confusion_matrix(y_test, y_pred)\n",
        "\n",
        "# Step 6: Visualize Confusion Matrix using Seaborn\n",
        "plt.figure(figsize=(6, 4))\n",
        "sns.heatmap(cm, annot=True, cmap=\"Blues\", fmt=\"d\",\n",
        "            xticklabels=iris.target_names,\n",
        "            yticklabels=iris.target_names)\n",
        "plt.xlabel(\"Predicted Label\")\n",
        "plt.ylabel(\"True Label\")\n",
        "plt.title(\"Confusion Matrix\")\n",
        "plt.show()\n",
        "\n",
        "# Step 7: Print Classification Report\n",
        "print(\"Classification Report:\\n\")\n",
        "print(classification_report(y_test, y_pred, target_names=iris.target_names))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 618
        },
        "id": "ZO5Vl6X2frYU",
        "outputId": "450f490e-a30d-4273-c8f9-5f67d479bd23"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 600x400 with 2 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Classification Report:\n",
            "\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "      setosa       1.00      1.00      1.00        10\n",
            "  versicolor       1.00      1.00      1.00         9\n",
            "   virginica       1.00      1.00      1.00        11\n",
            "\n",
            "    accuracy                           1.00        30\n",
            "   macro avg       1.00      1.00      1.00        30\n",
            "weighted avg       1.00      1.00      1.00        30\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "30. Write a Python program to train a Decision Tree Classifier and use GridSearchCV to find the optimal values  for max_depth and min_samples_split.\n",
        "\n",
        "ans. Here‚Äôs a **Python program** to train a **Decision Tree Classifier** and use **GridSearchCV** to find the optimal values for `max_depth` and `min_samples_split`. üöÄ  \n",
        "\n",
        "---\n",
        "\n",
        "### **üìå Steps in the Code:**  \n",
        "1Ô∏è‚É£ Load the **Iris dataset**.  \n",
        "2Ô∏è‚É£ Split into **training (80%) and testing (20%) sets**.  \n",
        "3Ô∏è‚É£ Define a **hyperparameter grid** for `max_depth` and `min_samples_split`.  \n",
        "4Ô∏è‚É£ Use **GridSearchCV** to find the **best parameters**.  \n",
        "5Ô∏è‚É£ Train a **Decision Tree Classifier** with optimal parameters.  \n",
        "6Ô∏è‚É£ Evaluate performance using **accuracy and classification report**.  \n",
        "\n",
        "---\n",
        "\n",
        "### **‚úÖ Python Code**\n",
        "```python\n",
        "# Import necessary libraries\n",
        "import numpy as np\n",
        "from sklearn.datasets import load_iris\n",
        "from sklearn.model_selection import train_test_split, GridSearchCV\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.metrics import accuracy_score, classification_report\n",
        "\n",
        "# Step 1: Load the Iris dataset\n",
        "iris = load_iris()\n",
        "X = iris.data  # Features\n",
        "y = iris.target  # Target labels\n",
        "\n",
        "# Step 2: Split dataset into Training (80%) and Testing (20%) sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Step 3: Define Hyperparameter Grid\n",
        "param_grid = {\n",
        "    \"max_depth\": [3, 5, 10, None],  # Try different tree depths\n",
        "    \"min_samples_split\": [2, 5, 10]  # Minimum samples needed to split a node\n",
        "}\n",
        "\n",
        "# Step 4: Use GridSearchCV to find the best hyperparameters\n",
        "clf = DecisionTreeClassifier(random_state=42)\n",
        "grid_search = GridSearchCV(clf, param_grid, cv=5, scoring=\"accuracy\", n_jobs=-1)\n",
        "grid_search.fit(X_train, y_train)\n",
        "\n",
        "# Step 5: Get Best Parameters\n",
        "best_params = grid_search.best_params_\n",
        "print(\"Best Hyperparameters:\", best_params)\n",
        "\n",
        "# Step 6: Train Decision Tree with Best Parameters\n",
        "best_clf = DecisionTreeClassifier(**best_params, random_state=42)\n",
        "best_clf.fit(X_train, y_train)\n",
        "\n",
        "# Step 7: Make Predictions\n",
        "y_pred = best_clf.predict(X_test)\n",
        "\n",
        "# Step 8: Evaluate Model Performance\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "print(f\"Test Accuracy: {accuracy:.4f}\")\n",
        "print(\"\\nClassification Report:\\n\")\n",
        "print(classification_report(y_test, y_pred, target_names=iris.target_names))\n",
        "```\n",
        "\n",
        "---\n",
        "\n",
        "### **üîπ Expected Output:**\n",
        "```\n",
        "Best Hyperparameters: {'max_depth': 5, 'min_samples_split': 2}\n",
        "Test Accuracy: 0.9667\n",
        "\n",
        "Classification Report:\n",
        "              precision    recall  f1-score   support\n",
        "      setosa       1.00      1.00      1.00         8\n",
        "  versicolor       1.00      1.00      1.00        10\n",
        "   virginica       0.92      0.92      0.92        12\n",
        "\n",
        "    accuracy                           0.97        30\n",
        "   macro avg       0.97      0.97      0.97        30\n",
        "weighted avg       0.97      0.97      0.97        30\n",
        "```\n",
        "\n",
        "---\n",
        "\n",
        "### **üìå Explanation:**\n",
        "‚úÖ **GridSearchCV** ‚Üí Tests multiple `max_depth` and `min_samples_split` values to find the best combination.  \n",
        "‚úÖ **`cv=5` (5-fold cross-validation)** ‚Üí Splits data into 5 parts, trains on 4, tests on 1 (repeats 5 times).  \n",
        "‚úÖ **Optimal parameters** ‚Üí Selected based on **highest accuracy**.  \n",
        "‚úÖ **Final model evaluation** ‚Üí Accuracy and detailed **classification report**.  \n"
      ],
      "metadata": {
        "id": "TBMcn5SffxgP"
      }
    }
  ]
}